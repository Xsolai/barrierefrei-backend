from typing import Dict, List, Any
import logging
import re
import traceback

class CriteriaMapper:
    def __init__(self):
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def _ensure_dict(self, obj: Any) -> bool:
        """
        Überprüft, ob ein Objekt ein Dictionary ist.
        Protokolliert eine Warnung, wenn nicht.
        
        Args:
            obj: Das zu überprüfende Objekt
            
        Returns:
            bool: True, wenn das Objekt ein Dictionary ist, sonst False
        """
        if not isinstance(obj, dict):
            self.logger.warning(f"Unerwarteter Typ: {type(obj)} für Objekt: {str(obj)[:100]}...")
            return False
        return True

    def map_data_to_criteria(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        Ordnet die extrahierten Daten den WCAG-Prüfpunkten zu
        """
        try:
            # Validiere Eingabedaten
            if not isinstance(crawler_data, dict) or not isinstance(accessibility_results, dict):
                raise ValueError(f"Ungültige Eingabe: crawler_data und accessibility_results müssen Dictionaries sein")
                
            # Prüfe, ob crawler_data die erwartete Struktur hat
            if not crawler_data.get("data"):
                self.logger.warning("crawler_data enthält keinen 'data'-Schlüssel")
                
            # Erstelle eine Kopie der Eingabedaten, um sicherzustellen, dass wir keine Seiteneffekte haben
            crawler_data_copy = {
                "data": {}
            }
            
            for url, page_data in crawler_data.get("data", {}).items():
                if isinstance(page_data, dict):
                    crawler_data_copy["data"][url] = page_data
                else:
                    self.logger.warning(f"Seiten-Daten für URL {url} sind kein Dictionary, werden übersprungen")
            
            # Erweitere mit den accessibility_results
            mapped_data = {
                "1_wahrnehmbar": self._map_perceivable_criteria(crawler_data_copy, accessibility_results),
                "2_bedienbar": self._map_operable_criteria(crawler_data_copy, accessibility_results),
                "3_verstaendlich": self._map_understandable_criteria(crawler_data_copy, accessibility_results),
                "4_robust": self._map_robust_criteria(crawler_data_copy, accessibility_results),
                "meta": {
                    "summary": self._generate_summary(crawler_data_copy, accessibility_results),
                    "scores": self._calculate_scores(crawler_data_copy, accessibility_results)
                }
            }
            
            return mapped_data
        except Exception as e:
            error_tb = traceback.format_exc()
            self.logger.error(f"Fehler bei der Zuordnung: {str(e)}")
            self.logger.error(error_tb)
            return {
                "error": str(e),
                "traceback": error_tb,
                "error_type": type(e).__name__
            }

    def _map_perceivable_criteria(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        1. Wahrnehmbar - Mapping der Prüfpunkte
        """
        return {
            "1.1_textalternativen": {
                "1.1.1_alternativtexte": {
                    "images": self._collect_image_data(crawler_data),
                    "aria_labels": self._collect_aria_labels(crawler_data),
                    "form_labels": self._collect_form_labels(crawler_data),
                    "text_alternatives": self._collect_text_alternatives(crawler_data),
                    "violations": self._get_violations(accessibility_results, "missing_alt"),
                    "icons_with_text": self._collect_icons_with_text(crawler_data)
                },
                "1.1.2_informative_grafiken": {
                    "complex_images": self._collect_complex_images(crawler_data),
                    "image_metadata": self._collect_image_metadata(crawler_data),
                    "svg_elements": self._collect_svg_elements(crawler_data)
                },
                "1.1.3_layout_grafiken": {
                    "decorative_images": self._collect_decorative_images(crawler_data)
                },
                "1.1.4_multimedia": {
                    "video_data": self._collect_video_data(crawler_data),
                    "audio_data": self._collect_audio_data(crawler_data)
                },
                "1.1.5_captcha": self._collect_captcha_data(crawler_data)
            },
            "1.2_zeitbasierte_medien": {
                "1.2.1_audio_transkripte": self._collect_audio_transcripts(crawler_data),
                "1.2.2_untertitel": self._collect_captions(crawler_data),
                "1.2.3_audiodeskription": self._collect_audio_descriptions(crawler_data)
            },
            "1.3_anpassbar": {
                "1.3.1_struktur": {
                    "html_structure": self._collect_html_structure(crawler_data),
                    "semantic_elements": self._collect_semantic_elements(crawler_data),
                    "landmarks": self._collect_landmarks(crawler_data),
                    "violations": self._get_violations(accessibility_results, "no_landmarks")
                },
                "1.3.2_ueberschriften": {
                    "heading_structure": self._collect_heading_structure(crawler_data),
                    "violations": self._get_violations(accessibility_results, "heading_hierarchy")
                },
                "1.3.3_layout": {
                    "css_layout": self._collect_css_layout(crawler_data),
                    "responsive_design": self._collect_responsive_data(crawler_data)
                },
                "1.3.4_reihenfolge": self._collect_content_order(crawler_data),
                "1.3.5_formular_zuordnung": {
                    "form_relationships": self._collect_form_relationships(crawler_data),
                    "form_labels": self._collect_detailed_form_labels(crawler_data),
                    "violations": self._get_violations(accessibility_results, "form_label")
                }
            },
            "1.4_unterscheidbar": {
                "1.4.1_farbe": self._collect_color_usage(crawler_data),
                "1.4.2_kontraste": {
                    "text_contrast": self._collect_text_contrast(crawler_data),
                    "ui_contrast": self._collect_ui_contrast(crawler_data),
                    "violations": self._get_violations(accessibility_results, "color_contrast")
                },
                "1.4.3_textgroesse": self._collect_text_sizing(crawler_data),
                "1.4.4_textanpassung": {
                    "text_adaptation": self._collect_text_adaptation(crawler_data),
                    "text_spacing": self._collect_text_spacing(crawler_data),
                    "violations": self._get_violations(accessibility_results, "text_spacing")
                },
                "1.4.5_grafische_kontraste": self._collect_graphical_contrast(crawler_data),
                "1.4.6_fokus": {
                    "focus_indicators": self._collect_focus_indicators(crawler_data),
                    "violations": self._get_violations(accessibility_results, "focus_not_visible")
                }
            }
        }

    def _map_operable_criteria(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        2. Bedienbar - Mapping der Prüfpunkte
        """
        return {
            "2.1_tastatur": {
                "2.1.1_tastaturzugang": {
                    "keyboard_access": self._collect_keyboard_access(crawler_data),
                    "interactive_elements": self._collect_interactive_elements(crawler_data),
                    "violations": self._get_violations(accessibility_results, "keyboard_navigation")
                },
                "2.1.2_tastatur_falle": {
                    "keyboard_traps": self._collect_keyboard_traps(crawler_data),
                    "focus_order": self._collect_focus_order(crawler_data)
                },
                "2.1.3_shortcuts": self._collect_keyboard_shortcuts(crawler_data)
            },
            "2.2_zeit": {
                "2.2.1_zeitbeschraenkungen": self._collect_time_limits(crawler_data),
                "2.2.2_pausen": self._collect_pause_features(crawler_data),
                "2.2.3_bewegung_blinken": self._collect_motion_animation(crawler_data)
            },
            "2.3_anfaelle": {
                "2.3.1_flackern": self._collect_flashing_content(crawler_data)
            },
            "2.4_navigation": {
                "2.4.1_skip_links": {
                    "skip_links": self._collect_skip_links(crawler_data),
                    "violations": self._get_violations(accessibility_results, "skip_links")
                },
                "2.4.2_seitentitel": {
                    "page_titles": self._collect_page_titles(crawler_data),
                    "violations": self._get_violations(accessibility_results, "missing_title")
                },
                "2.4.3_fokus_reihenfolge": self._collect_focus_order_data(crawler_data),
                "2.4.4_link_zweck": {
                    "link_purpose": self._collect_link_purpose(crawler_data),
                    "violations": self._get_violations(accessibility_results, "non_descriptive_link")
                },
                "2.4.5_zugriffsmethoden": self._collect_access_methods(crawler_data),
                "2.4.6_ueberschriften_beschriftungen": self._collect_headings_labels(crawler_data),
                "2.4.7_sichtbarer_fokus": {
                    "focus_visibility": self._collect_focus_visibility(crawler_data),
                    "violations": self._get_violations(accessibility_results, "focus_not_visible")
                }
            },
            "2.5_eingabemodalitaeten": {
                "2.5.1_zeigergesten": self._collect_pointer_gestures(crawler_data),
                "2.5.2_zeigerabbruch": self._collect_pointer_cancellation(crawler_data),
                "2.5.3_eingabe_zweck": self._collect_input_purpose(crawler_data),
                "2.5.4_bewegungssteuerung": self._collect_motion_actuation(crawler_data)
            }
        }

    def _map_understandable_criteria(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        3. Verständlich - Mapping der Prüfpunkte
        """
        return {
            "3.1_lesbar": {
                "3.1.1_dokumentsprache": {
                    "language_declaration": self._collect_language_declaration(crawler_data),
                    "violations": self._get_violations(accessibility_results, "language_missing")
                },
                "3.1.2_sprachwechsel": self._collect_language_changes(crawler_data)
            },
            "3.2_vorhersehbar": {
                "3.2.1_navigation_konsistent": {
                    "navigation_patterns": self._collect_navigation_patterns(crawler_data),
                    "menu_structure": self._collect_menu_structure(crawler_data)
                },
                "3.2.2_bezeichnungen_konsistent": self._collect_consistent_labeling(crawler_data)
            },
            "3.3_eingabe": {
                "3.3.1_fehlererkennung": {
                    "error_identification": self._collect_error_identification(crawler_data),
                    "form_validation": self._collect_form_validation(crawler_data),
                    "violations": self._get_violations(accessibility_results, "form_validation")
                },
                "3.3.2_beschriftungen": {
                    "form_labels_data": self._collect_form_labels_data(crawler_data),
                    "aria_labelledby": self._collect_aria_labelledby(crawler_data),
                    "violations": self._get_violations(accessibility_results, "form_label")
                },
                "3.3.3_fehlerbeschreibung": {
                    "error_descriptions": self._collect_error_descriptions(crawler_data),
                    "aria_errormessage": self._collect_aria_errormessage(crawler_data)
                },
                "3.3.4_hilfestellung": {
                    "help_data": self._collect_help_data(crawler_data),
                    "form_hints": self._collect_form_hints(crawler_data)
                }
            }
        }

    def _map_robust_criteria(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        4. Robust - Mapping der Prüfpunkte
        """
        return {
            "4.1_kompatibel": {
                "4.1.1_html_validierung": {
                    "html_validation": self._collect_html_validation(crawler_data),
                    "doctype": self._collect_doctype(crawler_data)
                },
                "4.1.2_name_rolle_wert": {
                    "aria_implementation": self._collect_aria_implementation(crawler_data),
                    "custom_controls": self._collect_custom_controls_data(crawler_data),
                    "violations": self._get_violations(accessibility_results, "invalid_aria_role")
                },
                "4.1.3_statusmeldungen": {
                    "status_messages": self._collect_status_messages(crawler_data),
                    "aria_live": self._collect_aria_live(crawler_data),
                    "role_alert": self._collect_role_alert(crawler_data)
                }
            }
        }

    # Hilfsmethoden für die Datensammlung
    def _collect_image_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt alle relevanten Bilddaten"""
        image_data = {
            "images_without_alt": [],
            "images_with_alt": [],
            "image_roles": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for image in page_data.get("structure", {}).get("images", []):
                if not self._ensure_dict(image):
                    continue
                    
                if not image.get("alt"):
                    image_data["images_without_alt"].append({
                        "url": url,
                        "image": image
                    })
                else:
                    image_data["images_with_alt"].append({
                        "url": url,
                        "image": image
                    })
                
                if image.get("role"):
                    image_data["image_roles"].append({
                        "url": url,
                        "image": image,
                        "role": image.get("role")
                    })
        
        return image_data

    def _collect_aria_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt ARIA-Label-Informationen"""
        aria_data = {
            "labeled_elements": [],
            "missing_labels": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for element in page_data.get("accessibility", {}).get("aria_labels", []):
                if not self._ensure_dict(element):
                    continue
                    
                if element.get("aria_attrs", {}).get("aria-label"):
                    aria_data["labeled_elements"].append({
                        "url": url,
                        "element": element
                    })
                else:
                    aria_data["missing_labels"].append({
                        "url": url,
                        "element": element
                    })
        
        return aria_data

    def _collect_form_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Formular-Label-Informationen"""
        form_data = {
            "labeled_fields": [],
            "unlabeled_fields": [],
            "aria_labeled_fields": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for form in page_data.get("structure", {}).get("forms", []):
                if not self._ensure_dict(form):
                    continue
                
                for field in form.get("fields", []):
                    if not self._ensure_dict(field):
                        continue
                        
                    if field.get("label"):
                        form_data["labeled_fields"].append({
                            "url": url,
                            "field": field
                        })
                    elif field.get("aria_label"):
                        form_data["aria_labeled_fields"].append({
                            "url": url,
                            "field": field
                        })
                    else:
                        form_data["unlabeled_fields"].append({
                            "url": url,
                            "field": field
                        })
        
        return form_data

    def _collect_complex_images(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über komplexe Bilder"""
        complex_images = {
            "infographics": [],
            "charts": [],
            "maps": [],
            "large_images": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for image in page_data.get("structure", {}).get("images", []):
                if not self._ensure_dict(image):
                    continue
                
                # Prüfe auf große Bilder
                try:
                    width = int(image.get("width", 0)) if image.get("width") else 0
                    height = int(image.get("height", 0)) if image.get("height") else 0
                    is_large_image = width * height > 100000  # Schwellenwert für große Bilder
                except (ValueError, TypeError):
                    self.logger.warning(f"_collect_complex_images - ungültige Bildgröße: width={image.get('width')}, height={image.get('height')}")
                    is_large_image = False
                
                if is_large_image:
                    complex_images["large_images"].append({
                        "url": url,
                        "image": image
                    })
                
                # Prüfe auf spezifische Bildtypen basierend auf Alt-Text oder Dateinamen
                alt_text = image.get("alt", "") or ""
                alt_text = alt_text.lower()
                src = image.get("src", "") or ""
                src = src.lower()
                
                if any(keyword in alt_text or keyword in src for keyword in ["chart", "graph", "diagramm", "grafik"]):
                    complex_images["charts"].append({
                        "url": url,
                        "image": image
                    })
                elif any(keyword in alt_text or keyword in src for keyword in ["map", "karte"]):
                    complex_images["maps"].append({
                        "url": url,
                        "image": image
                    })
                elif any(keyword in alt_text or keyword in src for keyword in ["infographic", "infografik"]):
                    complex_images["infographics"].append({
                        "url": url,
                        "image": image
                    })
        
        return complex_images

    def _collect_image_metadata(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Metadaten von Bildern"""
        metadata = {
            "dimensions": [],
            "file_types": [],
            "loading_attributes": [],
            "titles": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for image in page_data.get("structure", {}).get("images", []):
                if not isinstance(image, dict):
                    continue
                # Sammle Dimensionen
                if image.get("width") or image.get("height"):
                    metadata["dimensions"].append({
                        "url": url,
                        "image": image,
                        "width": image.get("width"),
                        "height": image.get("height")
                    })
                
                # Sammle Dateitypen
                src = image.get("src", "")
                if src:
                    file_type = src.split(".")[-1].lower() if "." in src else None
                    if file_type:
                        metadata["file_types"].append({
                            "url": url,
                            "image": image,
                            "type": file_type
                        })
                
                # Sammle Lazy-Loading-Attribute
                if image.get("loading"):
                    metadata["loading_attributes"].append({
                        "url": url,
                        "image": image,
                        "loading": image.get("loading")
                    })
                
                # Sammle Titel-Attribute
                if image.get("title"):
                    metadata["titles"].append({
                        "url": url,
                        "image": image,
                        "title": image.get("title")
                    })
        
        return metadata

    def _collect_decorative_images(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über dekorative Bilder"""
        decorative_images = {
            "empty_alt": [],
            "role_presentation": [],
            "background_images": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe normale Bilder
            for image in page_data.get("structure", {}).get("images", []):
                if not self._ensure_dict(image):
                    continue
                
                # Bilder mit leerem Alt-Text
                if image.get("alt") == "":
                    decorative_images["empty_alt"].append({
                        "url": url,
                        "image": image
                    })
                
                # Bilder mit Präsentationsrolle
                if image.get("role") == "presentation":
                    decorative_images["role_presentation"].append({
                        "url": url,
                        "image": image
                    })
            
            # Prüfe CSS-Hintergrundbilder
            for style in page_data.get("styling", {}).get("css_analysis", {}).get("inline_styles", []):
                if not self._ensure_dict(style):
                    continue
                    
                if "background-image" in str(style):
                    decorative_images["background_images"].append({
                        "url": url,
                        "style": style
                    })
        
        return decorative_images

    def _collect_video_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Videos"""
        video_data = {
            "with_captions": [],
            "without_captions": [],
            "with_descriptions": [],
            "without_descriptions": [],
            "controls": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for video in multimedia.get("video", []):
                if not self._ensure_dict(video):
                    continue
                
                # Prüfe Untertitel
                if video.get("captions"):
                    video_data["with_captions"].append({
                        "url": url,
                        "video": video
                    })
                else:
                    video_data["without_captions"].append({
                        "url": url,
                        "video": video
                    })
                
                # Prüfe Audiobeschreibungen
                if video.get("description"):
                    video_data["with_descriptions"].append({
                        "url": url,
                        "video": video
                    })
                else:
                    video_data["without_descriptions"].append({
                        "url": url,
                        "video": video
                    })
                
                # Prüfe Steuerelemente
                if video.get("controls"):
                    video_data["controls"].append({
                        "url": url,
                        "video": video
                    })
        
        return video_data

    def _collect_audio_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Audioinhalte"""
        audio_data = {
            "with_transcripts": [],
            "without_transcripts": [],
            "with_controls": [],
            "without_controls": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for audio in multimedia.get("audio", []):
                if not self._ensure_dict(audio):
                    continue
                
                # Prüfe Transkripte
                if audio.get("transcript"):
                    audio_data["with_transcripts"].append({
                        "url": url,
                        "audio": audio
                    })
                else:
                    audio_data["without_transcripts"].append({
                        "url": url,
                        "audio": audio
                    })
                
                # Prüfe Steuerelemente
                if audio.get("controls"):
                    audio_data["with_controls"].append({
                        "url": url,
                        "audio": audio
                    })
                else:
                    audio_data["without_controls"].append({
                        "url": url,
                        "audio": audio
                    })
        
        return audio_data

    def _collect_captcha_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über CAPTCHAs"""
        captcha_data = {
            "visual_captchas": [],
            "audio_alternatives": [],
            "other_alternatives": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Suche nach CAPTCHA-Elementen
            forms = page_data.get("structure", {}).get("forms", [])
            for form in forms:
                if not self._ensure_dict(form):
                    continue
                    
                for field in form.get("fields", []):
                    if not self._ensure_dict(field):
                        continue
                        
                    if any(captcha_term in str(field).lower() for captcha_term in ["captcha", "recaptcha", "verification"]):
                        # Prüfe auf verschiedene CAPTCHA-Typen
                        if field.get("type") == "image":
                            captcha_data["visual_captchas"].append({
                                "url": url,
                                "captcha": field
                            })
                        elif field.get("type") == "audio":
                            captcha_data["audio_alternatives"].append({
                                "url": url,
                                "captcha": field
                            })
                        else:
                            captcha_data["other_alternatives"].append({
                                "url": url,
                                "captcha": field
                            })
        
        return captcha_data

    def _collect_audio_transcripts(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Audio-Transkripte"""
        transcript_data = {
            "available_transcripts": [],
            "missing_transcripts": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for audio in multimedia.get("audio", []):
                if audio.get("transcript"):
                    transcript_data["available_transcripts"].append({
                        "url": url,
                        "audio": audio
                    })
                else:
                    transcript_data["missing_transcripts"].append({
                        "url": url,
                        "audio": audio
                    })
        
        return transcript_data

    def _collect_captions(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Untertitel"""
        caption_data = {
            "videos_with_captions": [],
            "videos_without_captions": [],
            "live_content": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for video in multimedia.get("video", []):
                if video.get("captions"):
                    caption_data["videos_with_captions"].append({
                        "url": url,
                        "video": video
                    })
                else:
                    caption_data["videos_without_captions"].append({
                        "url": url,
                        "video": video
                    })
                
                # Prüfe auf Live-Content
                if video.get("live"):
                    caption_data["live_content"].append({
                        "url": url,
                        "video": video
                    })
        
        return caption_data

    def _collect_audio_descriptions(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Audiobeschreibungen"""
        description_data = {
            "with_descriptions": [],
            "without_descriptions": [],
            "extended_descriptions": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for video in multimedia.get("video", []):
                if video.get("description"):
                    description_data["with_descriptions"].append({
                        "url": url,
                        "video": video
                    })
                else:
                    description_data["without_descriptions"].append({
                        "url": url,
                        "video": video
                    })
                
                # Prüfe auf erweiterte Beschreibungen
                if video.get("extended_description"):
                    description_data["extended_descriptions"].append({
                        "url": url,
                        "video": video
                    })
        
        return description_data

    def _collect_html_structure(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über die HTML-Struktur"""
        structure_data = {
            "semantic_elements": [],
            "non_semantic_elements": [],
            "landmarks": [],
            "sectioning": []
        }
        
        semantic_elements = ["header", "nav", "main", "article", "section", "aside", "footer"]
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Sammle semantische Elemente
            for element in semantic_elements:
                elements = page_data.get("structure", {}).get(element, [])
                if elements:
                    structure_data["semantic_elements"].extend([
                        {"url": url, "element": elem, "type": element}
                        for elem in elements
                    ])
            
            # Sammle Landmarks
            landmarks = page_data.get("structure", {}).get("landmarks", [])
            structure_data["landmarks"].extend([
                {"url": url, "landmark": landmark}
                for landmark in landmarks
            ])
            
            # Sammle Abschnittsstruktur
            sections = page_data.get("structure", {}).get("sectioning", [])
            structure_data["sectioning"].extend([
                {"url": url, "section": section}
                for section in sections
            ])
        
        return structure_data

    def _collect_semantic_elements(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über semantische Elemente"""
        semantic_data = {
            "headings": [],
            "lists": [],
            "tables": [],
            "forms": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Sammle Überschriften
            semantic_data["headings"].extend([
                {"url": url, "heading": heading}
                for heading in structure.get("headings", [])
            ])
            
            # Sammle Listen
            semantic_data["lists"].extend([
                {"url": url, "list": list_item}
                for list_item in structure.get("lists", [])
            ])
            
            # Sammle Tabellen
            semantic_data["tables"].extend([
                {"url": url, "table": table}
                for table in structure.get("tables", [])
            ])
            
            # Sammle Formulare
            semantic_data["forms"].extend([
                {"url": url, "form": form}
                for form in structure.get("forms", [])
            ])
        
        return semantic_data

    def _collect_heading_structure(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über die Überschriftenhierarchie"""
        heading_data = {
            "hierarchy": [],
            "missing_levels": [],
            "incorrect_order": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            headings = page_data.get("structure", {}).get("headings", [])
            
            # Sortiere Überschriften nach Position
            sorted_headings = sorted(headings, key=lambda h: h.get("position", 0))
            
            # Prüfe Hierarchie
            current_level = 0
            for heading in sorted_headings:
                level = heading.get("level", 0)
                
                # Füge zur Hierarchie hinzu
                heading_data["hierarchy"].append({
                    "url": url,
                    "heading": heading,
                    "level": level
                })
                
                # Prüfe auf fehlende Ebenen
                if level > current_level + 1:
                    heading_data["missing_levels"].append({
                        "url": url,
                        "heading": heading,
                        "missing_level": current_level + 1
                    })
                
                # Prüfe auf falsche Reihenfolge
                if level < current_level - 1:
                    heading_data["incorrect_order"].append({
                        "url": url,
                        "heading": heading,
                        "previous_level": current_level
                    })
                
                current_level = level
        
        return heading_data

    def _collect_css_layout(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über CSS-Layout"""
        layout_data = {
            "grid_layouts": [],
            "flex_layouts": [],
            "float_layouts": [],
            "positioning": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {}).get("css_analysis", {})
            
            # Analysiere CSS-Regeln
            for rule in styles.get("inline_styles", []):
                # Grid-Layout
                if any(prop in str(rule) for prop in ["display: grid", "grid-template"]):
                    layout_data["grid_layouts"].append({
                        "url": url,
                        "rule": rule
                    })
                
                # Flexbox-Layout
                if any(prop in str(rule) for prop in ["display: flex", "flex:"]):
                    layout_data["flex_layouts"].append({
                        "url": url,
                        "rule": rule
                    })
                
                # Float-Layout
                if "float:" in str(rule):
                    layout_data["float_layouts"].append({
                        "url": url,
                        "rule": rule
                    })
                
                # Positionierung
                if any(prop in str(rule) for prop in ["position:", "top:", "left:", "right:", "bottom:"]):
                    layout_data["positioning"].append({
                        "url": url,
                        "rule": rule
                    })
        
        return layout_data

    def _collect_responsive_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über responsives Design"""
        responsive_data = {
            "viewport": [],
            "media_queries": [],
            "responsive_images": [],
            "flexible_layouts": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Viewport-Metadaten
            meta_viewport = page_data.get("metadata", {}).get("viewport")
            if meta_viewport:
                responsive_data["viewport"].append({
                    "url": url,
                    "viewport": meta_viewport
                })
            
            # Media Queries
            styling = page_data.get("styling", {})
            media_queries = styling.get("responsive", {}).get("media_queries", [])
            responsive_data["media_queries"].extend([
                {"url": url, "query": query}
                for query in media_queries
            ])
            
            # Responsive Bilder
            for image in page_data.get("structure", {}).get("images", []):
                if image.get("srcset") or image.get("sizes"):
                    responsive_data["responsive_images"].append({
                        "url": url,
                        "image": image
                    })
            
            # Flexible Layouts
            css = styling.get("css_analysis", {})
            for style in css.get("inline_styles", []):
                if any(prop in str(style) for prop in ["flex", "grid", "%", "vw", "vh"]):
                    responsive_data["flexible_layouts"].append({
                        "url": url,
                        "style": style
                    })
        
        return responsive_data

    def _collect_content_order(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über die Inhaltsreihenfolge"""
        order_data = {
            "dom_order": [],
            "visual_order": [],
            "tab_order": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # DOM-Reihenfolge
            elements = page_data.get("structure", {}).get("semantic_elements", [])
            order_data["dom_order"].extend([
                {"url": url, "element": element}
                for element in elements
            ])
            
            # Visuelle Reihenfolge (z-index, position)
            styles = page_data.get("styling", {}).get("css_analysis", {})
            for style in styles.get("inline_styles", []):
                if any(prop in str(style) for prop in ["z-index:", "position:"]):
                    order_data["visual_order"].append({
                        "url": url,
                        "style": style
                    })
            
            # Tab-Reihenfolge
            tab_indices = page_data.get("accessibility", {}).get("tab_index", [])
            order_data["tab_order"].extend([
                {"url": url, "element": element}
                for element in tab_indices
            ])
        
        return order_data

    def _collect_form_relationships(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Formular-Beziehungen"""
        form_data = {
            "label_relationships": [],
            "group_relationships": [],
            "error_relationships": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            forms = page_data.get("structure", {}).get("forms", [])
            
            for form in forms:
                # Label-Beziehungen
                for field in form.get("fields", []):
                    if field.get("label") or field.get("aria_label"):
                        form_data["label_relationships"].append({
                            "url": url,
                            "field": field,
                            "label_type": "explicit" if field.get("label") else "aria"
                        })
                
                # Gruppierungsbeziehungen
                if form.get("fieldset") or form.get("role") == "group":
                    form_data["group_relationships"].append({
                        "url": url,
                        "form": form
                    })
                
                # Fehlerbeziehungen
                if form.get("aria-invalid") or form.get("aria-errormessage"):
                    form_data["error_relationships"].append({
                        "url": url,
                        "form": form
                    })
        
        return form_data

    def _collect_color_usage(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Farbverwendung"""
        color_data = {
            "text_colors": [],
            "background_colors": [],
            "border_colors": [],
            "semantic_colors": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # Textfarben
            for color in styles.get("colors", []):
                if "color:" in str(color):
                    color_data["text_colors"].append({
                        "url": url,
                        "color": color
                    })
            
            # Hintergrundfarben
            for color in styles.get("colors", []):
                if "background-color:" in str(color):
                    color_data["background_colors"].append({
                        "url": url,
                        "color": color
                    })
            
            # Rahmenfarben
            for color in styles.get("colors", []):
                if "border-color:" in str(color):
                    color_data["border_colors"].append({
                        "url": url,
                        "color": color
                    })
            
            # Semantische Farben (z.B. für Fehler, Erfolg)
            semantic_colors = {
                "error": ["red", "#ff0000"],
                "success": ["green", "#00ff00"],
                "warning": ["yellow", "#ffff00"]
            }
            
            for color in styles.get("colors", []):
                for semantic, values in semantic_colors.items():
                    if any(value in str(color) for value in values):
                        color_data["semantic_colors"].append({
                            "url": url,
                            "color": color,
                            "semantic_type": semantic
                        })
        
        return color_data

    def _collect_text_contrast(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textkontrast"""
        contrast_data = {
            "sufficient_contrast": [],
            "insufficient_contrast": [],
            "cannot_determine": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            for element in page_data.get("structure", {}).get("text_elements", []):
                if not self._ensure_dict(element):
                    continue
                
                # Versuche Kontrastverhältnis zu berechnen
                try:
                    contrast_ratio = self._calculate_contrast_ratio(
                        element.get("color"),
                        element.get("background-color")
                    )
                    
                    # Entscheide basierend auf WCAG-Richtlinien
                    if contrast_ratio >= 4.5:  # AA für normalen Text
                        contrast_data["sufficient_contrast"].append({
                            "url": url,
                            "element": element,
                            "ratio": contrast_ratio
                        })
                    else:
                        contrast_data["insufficient_contrast"].append({
                            "url": url,
                            "element": element,
                            "ratio": contrast_ratio
                        })
                except:
                    contrast_data["cannot_determine"].append({
                        "url": url,
                        "element": element
                    })
        
        return contrast_data

    def _calculate_contrast_ratio(self, color1: str, color2: str) -> float:
        """Berechnet das Kontrastverhältnis zwischen zwei Farben"""
        # Vereinfachte Implementierung - in der Praxis würde hier eine
        # vollständige Kontrastberechnung nach WCAG-Formel stehen
        return 4.5  # Beispielwert

    def _collect_ui_contrast(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über UI-Komponentenkontraste"""
        ui_contrast_data = {
            "active_controls": [],
            "inactive_controls": [],
            "focus_indicators": [],
            "graphical_objects": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe interaktive Elemente
            for element in page_data.get("structure", {}).get("interactive_elements", []):
                if not self._ensure_dict(element):
                    continue
                    
                # Aktive Steuerelemente
                if element.get("state") == "active":
                    ui_contrast_data["active_controls"].append({
                        "url": url,
                        "element": element,
                        "contrast": self._get_element_contrast(element)
                    })
                else:
                    ui_contrast_data["inactive_controls"].append({
                        "url": url,
                        "element": element,
                        "contrast": self._get_element_contrast(element)
                    })
            
            # Prüfe Fokus-Indikatoren
            styles = page_data.get("styling", {}).get("css_analysis", {})
            for style in styles.get("inline_styles", []):
                if not isinstance(style, dict) and not isinstance(style, str):
                    continue
                    
                if ":focus" in str(style):
                    ui_contrast_data["focus_indicators"].append({
                        "url": url,
                        "style": style
                    })
            
            # Prüfe grafische Objekte
            for image in page_data.get("structure", {}).get("images", []):
                if not self._ensure_dict(image):
                    continue
                    
                if image.get("role") == "presentation":
                    ui_contrast_data["graphical_objects"].append({
                        "url": url,
                        "image": image
                    })
        
        return ui_contrast_data

    def _get_element_contrast(self, element: Dict[str, Any]) -> float:
        """Berechnet den Kontrast eines Elements"""
        # Vereinfachte Implementierung - in der Praxis würde hier eine
        # vollständige Kontrastberechnung nach WCAG-Formel stehen
        return 3.0  # Beispielwert

    def _collect_text_sizing(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textgrößen"""
        sizing_data = {
            "absolute_sizes": [],
            "relative_sizes": [],
            "responsive_sizes": [],
            "zoom_behavior": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # Analysiere Schriftgrößen
            for style in styles.get("fonts", []):
                size = style.get("style", "")
                
                # Absolute Größen
                if any(unit in size for unit in ["px", "pt"]):
                    sizing_data["absolute_sizes"].append({
                        "url": url,
                        "style": style
                    })
                
                # Relative Größen
                elif any(unit in size for unit in ["em", "rem", "%"]):
                    sizing_data["relative_sizes"].append({
                        "url": url,
                        "style": style
                    })
                
                # Responsive Größen
                elif any(unit in size for unit in ["vw", "vh", "vmin", "vmax"]):
                    sizing_data["responsive_sizes"].append({
                        "url": url,
                        "style": style
                    })
            
            # Analysiere Zoom-Verhalten
            meta_viewport = page_data.get("metadata", {}).get("viewport", "")
            if "user-scalable=no" in meta_viewport or "maximum-scale" in meta_viewport:
                sizing_data["zoom_behavior"].append({
                    "url": url,
                    "viewport": meta_viewport,
                    "issue": "zoom_restricted"
                })
        
        return sizing_data

    def _collect_text_adaptation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textanpassungsmöglichkeiten"""
        adaptation_data = {
            "line_height": [],
            "letter_spacing": [],
            "word_spacing": [],
            "text_align": [],
            "text_transform": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # Analysiere Textanpassungen
            for style in styles.get("fonts", []):
                style_text = str(style.get("style", ""))
                
                # Zeilenhöhe
                if "line-height" in style_text:
                    adaptation_data["line_height"].append({
                        "url": url,
                        "style": style
                    })
                
                # Buchstabenabstand
                if "letter-spacing" in style_text:
                    adaptation_data["letter_spacing"].append({
                        "url": url,
                        "style": style
                    })
                
                # Wortabstand
                if "word-spacing" in style_text:
                    adaptation_data["word_spacing"].append({
                        "url": url,
                        "style": style
                    })
                
                # Textausrichtung
                if "text-align" in style_text:
                    adaptation_data["text_align"].append({
                        "url": url,
                        "style": style
                    })
                
                # Texttransformation
                if "text-transform" in style_text:
                    adaptation_data["text_transform"].append({
                        "url": url,
                        "style": style
                    })
        
        return adaptation_data

    def _collect_graphical_contrast(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über grafische Kontraste"""
        contrast_data = {
            "icons": [],
            "buttons": [],
            "borders": [],
            "backgrounds": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Analysiere Icons
            for image in page_data.get("structure", {}).get("images", []):
                if image.get("role") == "presentation" or "icon" in str(image.get("class", "")):
                    contrast_data["icons"].append({
                        "url": url,
                        "image": image,
                        "contrast": self._get_element_contrast(image)
                    })
            
            # Analysiere Buttons
            for button in page_data.get("structure", {}).get("interactive_elements", {}).get("buttons", []):
                contrast_data["buttons"].append({
                    "url": url,
                    "button": button,
                    "contrast": self._get_element_contrast(button)
                })
            
            # Analysiere Rahmen
            styles = page_data.get("styling", {})
            for style in styles.get("colors", []):
                if "border-color" in str(style):
                    contrast_data["borders"].append({
                        "url": url,
                        "style": style
                    })
            
            # Analysiere Hintergründe
            for style in styles.get("colors", []):
                if "background" in str(style):
                    contrast_data["backgrounds"].append({
                        "url": url,
                        "style": style
                    })
        
        return contrast_data

    def _collect_focus_indicators(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Fokus-Indikatoren"""
        focus_data = {
            "visible_focus": [],
            "hidden_focus": [],
            "custom_focus": [],
            "focus_styles": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # Analysiere Fokus-Stile
            for style in styles.get("css_analysis", {}).get("inline_styles", []):
                style_text = str(style)
                
                # Sichtbarer Fokus
                if ":focus" in style_text and "outline:" in style_text:
                    if "outline: none" in style_text or "outline: 0" in style_text:
                        focus_data["hidden_focus"].append({
                            "url": url,
                            "style": style
                        })
                    else:
                        focus_data["visible_focus"].append({
                            "url": url,
                            "style": style
                        })
                
                # Benutzerdefinierter Fokus
                if ":focus" in style_text and any(prop in style_text for prop in ["box-shadow", "border", "background"]):
                    focus_data["custom_focus"].append({
                        "url": url,
                        "style": style
                    })
            
            # Sammle alle Fokus-Stile
            focus_data["focus_styles"].extend([
                {"url": url, "style": style}
                for style in styles.get("css_analysis", {}).get("inline_styles", [])
                if ":focus" in str(style)
            ])
        
        return focus_data

    def _collect_keyboard_access(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Tastaturzugänglichkeit"""
        keyboard_data = {
            "focusable_elements": [],
            "non_focusable_elements": [],
            "custom_interactions": [],
            "keyboard_shortcuts": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Analysiere fokussierbare Elemente
            interactive = page_data.get("structure", {}).get("interactive_elements", {})
            
            for element in interactive.get("all", []):
                # Prüfe Fokussierbarkeit
                if element.get("tabindex") is not None:
                    keyboard_data["focusable_elements"].append({
                        "url": url,
                        "element": element
                    })
                else:
                    keyboard_data["non_focusable_elements"].append({
                        "url": url,
                        "element": element
                    })
            
            # Analysiere benutzerdefinierte Interaktionen
            scripts = page_data.get("scripting", {})
            for handler in scripts.get("event_handlers", []):
                if handler.get("type") in ["keydown", "keyup", "keypress"]:
                    keyboard_data["custom_interactions"].append({
                        "url": url,
                        "handler": handler
                    })
            
            # Analysiere Tastaturkürzel
            for element in interactive.get("all", []):
                if element.get("accesskey"):
                    keyboard_data["keyboard_shortcuts"].append({
                        "url": url,
                        "element": element,
                        "shortcut": element.get("accesskey")
                    })
        
        return keyboard_data

    def _collect_interactive_elements(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über interaktive Elemente"""
        interactive_data = {
            "buttons": [],
            "links": [],
            "form_controls": [],
            "custom_widgets": [],
            "event_handlers": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Sammle Buttons
            for button in structure.get("interactive_elements", {}).get("buttons", []):
                interactive_data["buttons"].append({
                    "url": url,
                    "button": button,
                    "accessible": self._check_element_accessibility(button)
                })
            
            # Sammle Links
            for link in structure.get("links", []):
                interactive_data["links"].append({
                    "url": url,
                    "link": link,
                    "accessible": self._check_element_accessibility(link)
                })
            
            # Sammle Formularsteuerelemente
            for form in structure.get("forms", []):
                for control in form.get("fields", []):
                    interactive_data["form_controls"].append({
                        "url": url,
                        "control": control,
                        "accessible": self._check_element_accessibility(control)
                    })
            
            # Sammle benutzerdefinierte Widgets
            for widget in structure.get("interactive_elements", {}).get("custom_controls", []):
                interactive_data["custom_widgets"].append({
                    "url": url,
                    "widget": widget,
                    "accessible": self._check_element_accessibility(widget)
                })
            
            # Sammle Event-Handler
            scripts = page_data.get("scripting", {})
            for handler in scripts.get("event_handlers", []):
                interactive_data["event_handlers"].append({
                    "url": url,
                    "handler": handler
                })
        
        return interactive_data

    def _check_element_accessibility(self, element: Dict[str, Any]) -> Dict[str, bool]:
        """Prüft die Zugänglichkeit eines Elements"""
        if not self._ensure_dict(element):
            return {
                "has_name": False,
                "has_role": False,
                "is_focusable": False,
                "has_keyboard_event": False,
                "has_state": False
            }
            
        return {
            "has_name": bool(element.get("aria-label") or element.get("aria-labelledby") or element.get("title")),
            "has_role": bool(element.get("role")),
            "is_focusable": element.get("tabindex") is not None,
            "has_keyboard_event": bool(element.get("keydown") or element.get("keyup") or element.get("keypress")),
            "has_state": bool(element.get("aria-expanded") or element.get("aria-pressed") or element.get("aria-checked"))
        }

    def _collect_keyboard_trap_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Tastaturfallen"""
        trap_data = {
            "potential_traps": [],
            "modal_dialogs": [],
            "custom_widgets": [],
            "focus_management": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Prüfe auf potenzielle Tastaturfallen
            for element in structure.get("interactive_elements", {}).get("all", []):
                if element.get("tabindex") == "-1":
                    trap_data["potential_traps"].append({
                        "url": url,
                        "element": element
                    })
            
            # Prüfe modale Dialoge
            for dialog in structure.get("interactive_elements", {}).get("dialogs", []):
                if not dialog.get("aria-modal"):
                    trap_data["modal_dialogs"].append({
                        "url": url,
                        "dialog": dialog
                    })
            
            # Prüfe benutzerdefinierte Widgets
            for widget in structure.get("interactive_elements", {}).get("custom_controls", []):
                if not self._has_proper_focus_management(widget):
                    trap_data["custom_widgets"].append({
                        "url": url,
                        "widget": widget
                    })
            
            # Prüfe Fokus-Management
            scripts = page_data.get("scripting", {})
            for handler in scripts.get("event_handlers", []):
                if "focus" in str(handler):
                    trap_data["focus_management"].append({
                        "url": url,
                        "handler": handler
                    })
        
        return trap_data

    def _has_proper_focus_management(self, widget: Dict[str, Any]) -> bool:
        """Prüft, ob ein Widget korrektes Fokus-Management implementiert"""
        if not self._ensure_dict(widget):
            return False
            
        # Prüfe auf tabindex
        if widget.get("tabindex") == "-1":
            return False

        # Prüfe auf Keyboard-Event-Handler
        has_keyboard_events = bool(
            widget.get("keydown") or 
            widget.get("keyup") or 
            widget.get("keypress")
        )

        return has_keyboard_events

    def _collect_timing_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Zeitsteuerung"""
        timing_data = {
            "auto_updates": [],
            "timeouts": [],
            "animations": [],
            "carousels": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe auf automatische Aktualisierungen
            meta_refresh = page_data.get("metadata", {}).get("refresh")
            if meta_refresh:
                timing_data["auto_updates"].append({
                    "url": url,
                    "refresh": meta_refresh
                })
            
            # Prüfe auf Timeouts
            scripts = page_data.get("scripting", {})
            inline_scripts = scripts.get("inline_scripts", [])
            
            # Stelle sicher, dass inline_scripts iterierbar ist
            if not isinstance(inline_scripts, (list, tuple, set, dict)):
                self.logger.warning(f"inline_scripts ist kein iterierbares Objekt: {type(inline_scripts)}")
                continue
                
            for script in inline_scripts:
                if not isinstance(script, (dict, str)):
                    continue
                    
                if "setTimeout" in str(script) or "setInterval" in str(script):
                    timing_data["timeouts"].append({
                        "url": url,
                        "script": script
                    })
            
            # Prüfe auf Animationen
            styles = page_data.get("styling", {})
            animations = styles.get("css_analysis", {}).get("animations", [])
            
            # Stelle sicher, dass animations iterierbar ist
            if not isinstance(animations, (list, tuple, set, dict)):
                self.logger.warning(f"animations ist kein iterierbares Objekt: {type(animations)}")
            else:
                for style in animations:
                    if not isinstance(style, (dict, str)):
                        continue
                        
                    timing_data["animations"].append({
                        "url": url,
                        "animation": style
                    })
            
            # Prüfe auf Karussells
            custom_controls = page_data.get("structure", {}).get("interactive_elements", {}).get("custom_controls", [])
            
            # Stelle sicher, dass custom_controls iterierbar ist
            if not isinstance(custom_controls, (list, tuple, set, dict)):
                self.logger.warning(f"custom_controls ist kein iterierbares Objekt: {type(custom_controls)}")
            else:
                for element in custom_controls:
                    if not self._ensure_dict(element):
                        continue
                        
                    if "carousel" in str(element.get("class", "")) or "slider" in str(element.get("class", "")):
                        timing_data["carousels"].append({
                            "url": url,
                            "carousel": element
                        })
        
        return timing_data

    def _collect_pausable_content(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über pausierbaren Inhalt"""
        pausable_data = {
            "moving_content": [],
            "auto_updating_content": [],
            "media_players": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe bewegte Inhalte
            styles = page_data.get("styling", {})
            animations = styles.get("css_analysis", {}).get("animations", [])
            
            # Stelle sicher, dass animations iterierbar ist
            if not isinstance(animations, (list, tuple, set, dict)):
                self.logger.warning(f"animations ist kein iterierbares Objekt: {type(animations)}")
            else:
                for animation in animations:
                    if not isinstance(animation, (dict, str)):
                        continue
                        
                    pausable_data["moving_content"].append({
                        "url": url,
                        "animation": animation,
                        "has_controls": self._has_animation_controls(animation)
                    })
            
            # Prüfe sich automatisch aktualisierende Inhalte
            scripts = page_data.get("scripting", {})
            inline_scripts = scripts.get("inline_scripts", [])
            
            # Stelle sicher, dass inline_scripts iterierbar ist
            if not isinstance(inline_scripts, (list, tuple, set, dict)):
                self.logger.warning(f"inline_scripts ist kein iterierbares Objekt: {type(inline_scripts)}")
            else:
                for script in inline_scripts:
                    if not isinstance(script, (dict, str)):
                        continue
                        
                    if "setInterval" in str(script):
                        pausable_data["auto_updating_content"].append({
                            "url": url,
                            "script": script
                        })
            
            # Prüfe Medienplayer
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            videos = multimedia.get("video", [])
            audios = multimedia.get("audio", [])
            
            # Stelle sicher, dass videos und audios iterierbar sind
            all_media = []
            if isinstance(videos, (list, tuple)):
                all_media.extend(videos)
            if isinstance(audios, (list, tuple)):
                all_media.extend(audios)
                
            for media in all_media:
                if not self._ensure_dict(media):
                    continue
                    
                if media.get("controls"):
                    pausable_data["media_players"].append({
                        "url": url,
                        "media": media
                    })
        
        return pausable_data

    def _has_animation_controls(self, animation: Dict[str, Any]) -> bool:
        """Prüft, ob eine Animation Steuerelemente hat"""
        animation_text = str(animation)
        return (
            "animation-play-state" in animation_text or
            "animation-duration" in animation_text or
            "animation-delay" in animation_text
        )

    def _collect_animation_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Animationen"""
        animation_data = {
            "css_animations": [],
            "transitions": [],
            "animated_gifs": [],
            "video_animations": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # CSS-Animationen
            for style in styles.get("css_analysis", {}).get("animations", []):
                if "@keyframes" in str(style):
                    animation_data["css_animations"].append({
                        "url": url,
                        "animation": style,
                        "has_controls": self._has_animation_controls(style)
                    })
            
            # CSS-Transitionen
            for style in styles.get("css_analysis", {}).get("transitions", []):
                animation_data["transitions"].append({
                    "url": url,
                    "transition": style
                })
            
            # Animierte GIFs
            for image in page_data.get("structure", {}).get("images", []):
                if image.get("src", "").lower().endswith(".gif"):
                    animation_data["animated_gifs"].append({
                        "url": url,
                        "image": image
                    })
            
            # Video-Animationen
            multimedia = page_data.get("structure", {}).get("multimedia", {})
            for video in multimedia.get("video", []):
                if video.get("autoplay"):
                    animation_data["video_animations"].append({
                        "url": url,
                        "video": video
                    })
        
        return animation_data

    def _collect_flashing_content(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über blinkende Inhalte"""
        flashing_content = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Animationen
            animations = scripting.get("animations", [])
            for animation in animations:
                if animation.get("frequency") and animation.get("frequency") > 2:
                    flashing_content.append({
                        "url": url,
                        "type": "animation",
                        "element_id": animation.get("id"),
                        "frequency": animation.get("frequency")
                    })
                    
            # GIF-Bilder
            structure = page_data.get("structure", {})
            for img in structure.get("images", []):
                if img.get("src") and img.get("src").lower().endswith(".gif"):
                    flashing_content.append({
                        "url": url,
                        "type": "gif_image",
                        "src": img.get("src")
                    })
                    
        return flashing_content

    def _collect_pointer_gestures(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeigergesten"""
        gestures = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die spezifische Gesten erfordern könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type in ["mousedown", "mouseup", "mousemove", "dragstart", "drag", "drop"]:
                    gestures.append({
                        "url": url,
                        "type": "mouse_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                
                if event_type in ["touchstart", "touchmove", "touchend", "swipe"]:
                    gestures.append({
                        "url": url,
                        "type": "touch_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                    
        return gestures

    def _collect_pointer_cancellation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeiger-Abbruchmöglichkeiten"""
        cancellation_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die potenziell problematisch sein könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type == "mousedown" and not any(e.get("type") == "mouseup" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "mousedown_without_mouseup",
                        "element": event.get("element")
                    })
                    
                if event_type == "touchstart" and not any(e.get("type") == "touchend" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "touchstart_without_touchend",
                        "element": event.get("element")
                    })
                    
        return cancellation_data

    def _collect_input_purpose(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über den Zweck von Eingabefeldern"""
        input_purpose_data = {
            "with_autocomplete": [],
            "with_type": [],
            "with_aria_purpose": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Felder mit autocomplete-Attribut
                    if field.get("autocomplete"):
                        input_purpose_data["with_autocomplete"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "autocomplete": field.get("autocomplete")
                        })
                        
                    # Felder mit spezifischem Typ
                    if field.get("type") in ["email", "tel", "url", "number", "date", "time", "datetime-local"]:
                        input_purpose_data["with_type"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "type": field.get("type")
                        })
                        
                    # Felder mit ARIA-Attributen für den Zweck
                    aria_attrs = field.get("aria_attributes", {})
                    if "aria-autocomplete" in aria_attrs:
                        input_purpose_data["with_aria_purpose"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "aria_autocomplete": aria_attrs.get("aria-autocomplete")
                        })
                        
        return input_purpose_data

    def _collect_motion_actuation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Bewegungssteuerung"""
        motion_actuation = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach Ereignissen, die auf Bewegungssteuerung hindeuten könnten
            events = scripting.get("event_handlers", [])
            for event in events:
                if event.get("type") in ["devicemotion", "deviceorientation", "shake", "tilt"]:
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_event",
                        "event_type": event.get("type"),
                        "element": event.get("element")
                    })
                    
            # Suche nach JavaScript-APIs für Bewegung
            scripts = scripting.get("scripts", [])
            for script in scripts:
                content = str(script.get("content", ""))
                if any(api in content for api in ["DeviceMotionEvent", "DeviceOrientationEvent", "Gyroscope", "Accelerometer"]):
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_api",
                        "script_id": script.get("id"),
                        "content_snippet": content[:100] if len(content) > 100 else content
                    })
                    
        return motion_actuation

    def _collect_status_messages(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Statusmeldungen"""
        status_messages = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach typischen Status-Elementen
            selectors = [
                "[role='status']", 
                "[role='alert']", 
                "[aria-live]", 
                ".status", 
                ".message", 
                ".notification"
            ]
            
            for selector in selectors:
                elements = []
                
                # Simuliere CSS-Selektor-Auswahl
                if selector.startswith("["):
                    attr = selector[1:-1].split("=")
                    if len(attr) == 2:
                        attr_name = attr[0]
                        attr_value = attr[1].strip("'\"")
                        
                        for form in structure.get("forms", []):
                            for field in form.get("fields", []):
                                aria_attrs = field.get("aria_attributes", {})
                                if aria_attrs.get(attr_name) == attr_value:
                                    elements.append(field)
                elif selector.startswith("."):
                    class_name = selector[1:]
                    
                    for form in structure.get("forms", []):
                        for field in form.get("fields", []):
                            if class_name in field.get("classes", []):
                                elements.append(field)
                
                for element in elements:
                    status_messages.append({
                        "url": url,
                        "selector": selector,
                        "element": element
                    })
                    
        return status_messages

    def _generate_summary(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generiert eine Zusammenfassung der Barrierefreiheitsanalyse"""
        summary = {
            "urls_analyzed": len(crawler_data.get("data", {})),
            "violations_count": 0,
            "warnings_count": 0,
            "passed_count": 0,
            "top_issues": [],
            "positive_aspects": []
        }
        
        # Zähle Verstöße, Warnungen und bestandene Tests
        if accessibility_results:
            summary["violations_count"] = len(accessibility_results.get("violations", []))
            summary["warnings_count"] = len(accessibility_results.get("warnings", []))
            summary["passed_count"] = len(accessibility_results.get("passed", []))
            
            # Gruppiere Verstöße nach Typ
            violation_types = {}
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                if v_type not in violation_types:
                    violation_types[v_type] = 0
                violation_types[v_type] += 1
                
            # Top-Probleme
            top_issues = sorted(violation_types.items(), key=lambda x: x[1], reverse=True)
            summary["top_issues"] = [{"type": t, "count": c} for t, c in top_issues[:5]]
            
            # Positive Aspekte
            passed_types = {}
            for p in accessibility_results.get("passed", []):
                p_type = p.get("type", "unknown")
                if p_type not in passed_types:
                    passed_types[p_type] = 0
                passed_types[p_type] += 1
                
            top_passed = sorted(passed_types.items(), key=lambda x: x[1], reverse=True)
            summary["positive_aspects"] = [{"type": t, "count": c} for t, c in top_passed[:5]]
            
        return summary

    def _calculate_scores(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, float]:
        """Berechnet Barrierefreiheits-Scores für verschiedene Kategorien"""
        scores = {
            "perceivable": 0.0,
            "operable": 0.0,
            "understandable": 0.0,
            "robust": 0.0,
            "overall": 0.0
        }
        
        # Gewichtung nach Schweregrad
        violation_weight = 1.0
        warning_weight = 0.5
        
        # Kategorisiere Verstöße und Warnungen
        if accessibility_results:
            categories = {
                "perceivable": ["missing_alt", "color_contrast", "text_spacing"],
                "operable": ["keyboard_navigation", "skip_links", "focus_not_visible", "non_descriptive_link"],
                "understandable": ["language_missing", "form_label", "form_validation"],
                "robust": ["invalid_aria_role", "heading_hierarchy", "no_landmarks"]
            }
            
            # Zähle Verstöße pro Kategorie
            category_violations = {k: 0 for k in scores.keys()}
            category_warnings = {k: 0 for k in scores.keys()}
            
            # Verstöße
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                for category, types in categories.items():
                    if v_type in types:
                        category_violations[category] += 1
                        break
            
            # Warnungen
            for w in accessibility_results.get("warnings", []):
                w_type = w.get("type", "unknown")
                for category, types in categories.items():
                    if w_type in types:
                        category_warnings[category] += 1
                        break
            
            # Berechne Score pro Kategorie (0-100, höher ist besser)
            max_issues = 10  # Annahme: Bei mehr als 10 Verstößen ist der Score 0
            
            for category in scores.keys():
                if category != "overall":
                    weighted_issues = (category_violations[category] * violation_weight + 
                                      category_warnings[category] * warning_weight)
                    
                    # Score-Berechnung: 100 - (gewichtete Probleme / max_issues * 100)
                    # Begrenzt auf 0-100
                    raw_score = 100 - (weighted_issues / max_issues * 100)
                    scores[category] = max(0, min(100, raw_score))
            
            # Gesamtscore: Durchschnitt der Kategorien
            scores["overall"] = sum(v for k, v in scores.items() if k != "overall") / 4
            
        return scores

    def _collect_text_alternatives(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Text-Alternativen für nicht-textuelle Inhalte"""
        alternatives_data = {
            "images_without_alt": [],
            "decorative_images": [],
            "complex_images": [],
            "svg_elements": [],
            "canvas_elements": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in alternatives_data.keys():
                if key in text_alts:
                    for item in text_alts[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        alternatives_data[key].append(item_copy)
                        
        return alternatives_data

    def _collect_icons_with_text(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Icons mit Text"""
        icons_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "icons_with_text" in text_alts:
                for icon in text_alts["icons_with_text"]:
                    icon_copy = icon.copy()
                    icon_copy["url"] = url
                    icons_data.append(icon_copy)
                    
        return icons_data

    def _collect_svg_elements(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über SVG-Elemente"""
        svg_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "svg_elements" in text_alts:
                for svg in text_alts["svg_elements"]:
                    svg_copy = svg.copy()
                    svg_copy["url"] = url
                    svg_data.append(svg_copy)
                    
        return svg_data

    def _collect_landmarks(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Landmarks"""
        landmarks_data = {
            "navigation": [],
            "main": [],
            "complementary": [],
            "banner": [],
            "contentinfo": [],
            "search": [],
            "form": [],
            "region": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            landmarks = structure.get("landmarks", [])
            
            for landmark in landmarks:
                role = landmark.get("role")
                if role in landmarks_data:
                    landmark_copy = landmark.copy()
                    landmark_copy["url"] = url
                    landmarks_data[role].append(landmark_copy)
                    
        return landmarks_data

    def _collect_detailed_form_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt detaillierte Informationen über Formularfeld-Labels"""
        form_labels = {
            "with_label": [],
            "with_aria_label": [],
            "with_placeholder": [],
            "without_label": [],
            "grouped_fields": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    field_data = {
                        "form_id": form.get("id"),
                        "field_id": field.get("id"),
                        "field_type": field.get("type"),
                        "url": url
                    }
                    
                    # Kategorisiere nach Labeltyp
                    if field.get("label"):
                        field_data["label"] = field["label"]
                        form_labels["with_label"].append(field_data)
                    elif field.get("aria_attributes", {}).get("aria-label"):
                        field_data["aria_label"] = field["aria_attributes"]["aria-label"]
                        form_labels["with_aria_label"].append(field_data)
                    elif field.get("placeholder"):
                        field_data["placeholder"] = field["placeholder"]
                        form_labels["with_placeholder"].append(field_data)
                    else:
                        form_labels["without_label"].append(field_data)
                        
                    # Gruppierte Felder
                    if field.get("grouped"):
                        group_data = field_data.copy()
                        group_data["group_info"] = field["grouped"]
                        form_labels["grouped_fields"].append(group_data)
                        
        return form_labels

    def _collect_text_spacing(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textabstände"""
        spacing_data = {
            "line_height": [],
            "letter_spacing": [],
            "word_spacing": [],
            "text_indent": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styling = page_data.get("styling", {})
            text_spacing = styling.get("text_spacing", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in spacing_data.keys():
                if key in text_spacing:
                    for value in text_spacing[key]:
                        spacing_data[key].append({
                            "value": value,
                            "url": url
                        })
                        
        return spacing_data

    def _collect_keyboard_traps(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über potenzielle Keyboard-Traps"""
        trap_data = {
            "custom_widgets": [],
            "modal_dialogs": [],
            "dropdown_menus": [],
            "negative_tabindex": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            keyboard_navigation = accessibility.get("keyboard_navigation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in trap_data.keys():
                if key in keyboard_navigation:
                    for item in keyboard_navigation[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        trap_data[key].append(item_copy)
                        
        return trap_data

    def _collect_form_validation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Formularvalidierung"""
        validation_data = {
            "html5_validation": [],
            "custom_validation": [],
            "pattern_validation": [],
            "constraint_validation": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            form_validation = accessibility.get("form_validation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in validation_data.keys():
                if key in form_validation:
                    # Für Listen von Objekten
                    if isinstance(form_validation[key], list):
                        for item in form_validation[key]:
                            item_copy = item.copy()
                            item_copy["url"] = url
                            validation_data[key].append(item_copy)
                    # Für einzelne Objekte
                    elif isinstance(form_validation[key], dict):
                        item_copy = form_validation[key].copy()
                        item_copy["url"] = url
                        validation_data[key].append(item_copy)
                        
        return validation_data

    def _collect_aria_labelledby(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-labelledby-Attribute"""
        labelledby_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-labelledby" in aria_attributes:
                for item in aria_attributes["aria-labelledby"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    labelledby_data.append(item_copy)
                    
        return labelledby_data

    def _collect_aria_errormessage(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-errormessage-Attribute"""
        errormessage_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-errormessage" in aria_attributes:
                for item in aria_attributes["aria-errormessage"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    errormessage_data.append(item_copy)
                    
        return errormessage_data

    def _collect_form_hints(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Formularfeld-Hilfetexte"""
        hint_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    if field.get("help_text"):
                        hint_data.append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "field_type": field.get("type"),
                            "help_text": field.get("help_text")
                        })
                        
        return hint_data

    def _collect_doctype(self, crawler_data: Dict[str, Any]) -> Dict[str, str]:
        """Sammelt Informationen über Doctype-Deklarationen"""
        doctype_data = {}
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            doctype = structure.get("doctype")
            
            if doctype:
                doctype_data[url] = doctype
                
        return doctype_data

    def _collect_aria_live(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-live-Regionen"""
        live_regions = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-live" in aria_attributes:
                for item in aria_attributes["aria-live"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    live_regions.append(item_copy)
                    
        return live_regions

    def _collect_role_alert(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Elemente mit role='alert'"""
        alerts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_roles = accessibility.get("aria_roles", [])
            
            for role in aria_roles:
                if role.get("role") == "alert":
                    role_copy = role.copy()
                    role_copy["url"] = url
                    alerts.append(role_copy)
                    
        return alerts

    def _collect_keyboard_shortcuts(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Tastaturkürzel"""
        shortcuts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            for event in events:
                if event.get("type", "").startswith("key"):
                    shortcuts.append({
                        "url": url,
                        "event_type": event.get("type"),
                        "element": event.get("element"),
                        "id": event.get("id")
                    })
                    
        return shortcuts

    def _collect_time_limits(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeitbeschränkungen"""
        time_limits = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach setTimeout und setInterval
            for script in scripting.get("scripts", []):
                content = str(script.get("content", ""))
                
                if "setTimeout" in content or "setInterval" in content:
                    time_limits.append({
                        "url": url,
                        "type": "timeout_interval",
                        "script_id": script.get("id"),
                        "has_settimeout": "setTimeout" in content,
                        "has_setinterval": "setInterval" in content
                    })
                    
        return time_limits

    def _collect_pause_features(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Pause-Funktionen"""
        pause_features = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach Pause-Buttons in Multimedia-Elementen
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "video_controls",
                        "element_id": video.get("id"),
                        "has_controls": True
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "audio_controls",
                        "element_id": audio.get("id"),
                        "has_controls": True
                    })
                    
            # Suche nach Pause-Buttons für Animationen
            animations = page_data.get("scripting", {}).get("animations", [])
            for animation in animations:
                if animation.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "animation_controls",
                        "element_id": animation.get("id"),
                        "has_controls": True
                    })
                    
        return pause_features

    def _collect_motion_animation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Bewegung und Animation"""
        motion_data = {
            "animations": [],
            "transitions": [],
            "autoplay_media": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            structure = page_data.get("structure", {})
            
            # CSS-Animationen
            css = page_data.get("styling", {}).get("css_analysis", {})
            
            for rule in css.get("rules", []):
                declarations = rule.get("declarations", {})
                
                if "animation" in declarations or "animation-name" in declarations:
                    motion_data["animations"].append({
                        "url": url,
                        "type": "css_animation",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("animation")}
                    })
                    
                if "transition" in declarations or "transition-property" in declarations:
                    motion_data["transitions"].append({
                        "url": url,
                        "type": "css_transition",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("transition")}
                    })
                    
            # Autoplay-Medien
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_video",
                        "element_id": video.get("id")
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_audio",
                        "element_id": audio.get("id")
                    })
                    
        return motion_data

    def _collect_flashing_content(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über blinkende Inhalte"""
        flashing_content = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Animationen
            animations = scripting.get("animations", [])
            for animation in animations:
                if animation.get("frequency") and animation.get("frequency") > 2:
                    flashing_content.append({
                        "url": url,
                        "type": "animation",
                        "element_id": animation.get("id"),
                        "frequency": animation.get("frequency")
                    })
                    
            # GIF-Bilder
            structure = page_data.get("structure", {})
            for img in structure.get("images", []):
                if img.get("src") and img.get("src").lower().endswith(".gif"):
                    flashing_content.append({
                        "url": url,
                        "type": "gif_image",
                        "src": img.get("src")
                    })
                    
        return flashing_content

    def _collect_pointer_gestures(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeigergesten"""
        gestures = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die spezifische Gesten erfordern könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type in ["mousedown", "mouseup", "mousemove", "dragstart", "drag", "drop"]:
                    gestures.append({
                        "url": url,
                        "type": "mouse_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                
                if event_type in ["touchstart", "touchmove", "touchend", "swipe"]:
                    gestures.append({
                        "url": url,
                        "type": "touch_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                    
        return gestures

    def _collect_pointer_cancellation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeiger-Abbruchmöglichkeiten"""
        cancellation_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die potenziell problematisch sein könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type == "mousedown" and not any(e.get("type") == "mouseup" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "mousedown_without_mouseup",
                        "element": event.get("element")
                    })
                    
                if event_type == "touchstart" and not any(e.get("type") == "touchend" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "touchstart_without_touchend",
                        "element": event.get("element")
                    })
                    
        return cancellation_data

    def _collect_input_purpose(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über den Zweck von Eingabefeldern"""
        input_purpose_data = {
            "with_autocomplete": [],
            "with_type": [],
            "with_aria_purpose": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Felder mit autocomplete-Attribut
                    if field.get("autocomplete"):
                        input_purpose_data["with_autocomplete"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "autocomplete": field.get("autocomplete")
                        })
                        
                    # Felder mit spezifischem Typ
                    if field.get("type") in ["email", "tel", "url", "number", "date", "time", "datetime-local"]:
                        input_purpose_data["with_type"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "type": field.get("type")
                        })
                        
                    # Felder mit ARIA-Attributen für den Zweck
                    aria_attrs = field.get("aria_attributes", {})
                    if "aria-autocomplete" in aria_attrs:
                        input_purpose_data["with_aria_purpose"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "aria_autocomplete": aria_attrs.get("aria-autocomplete")
                        })
                        
        return input_purpose_data

    def _collect_motion_actuation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Bewegungssteuerung"""
        motion_actuation = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach Ereignissen, die auf Bewegungssteuerung hindeuten könnten
            events = scripting.get("event_handlers", [])
            for event in events:
                if event.get("type") in ["devicemotion", "deviceorientation", "shake", "tilt"]:
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_event",
                        "event_type": event.get("type"),
                        "element": event.get("element")
                    })
                    
            # Suche nach JavaScript-APIs für Bewegung
            scripts = scripting.get("scripts", [])
            for script in scripts:
                content = str(script.get("content", ""))
                if any(api in content for api in ["DeviceMotionEvent", "DeviceOrientationEvent", "Gyroscope", "Accelerometer"]):
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_api",
                        "script_id": script.get("id"),
                        "content_snippet": content[:100] if len(content) > 100 else content
                    })
                    
        return motion_actuation

    def _collect_status_messages(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Statusmeldungen"""
        status_messages = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach typischen Status-Elementen
            selectors = [
                "[role='status']", 
                "[role='alert']", 
                "[aria-live]", 
                ".status", 
                ".message", 
                ".notification"
            ]
            
            for selector in selectors:
                elements = []
                
                # Simuliere CSS-Selektor-Auswahl
                if selector.startswith("["):
                    attr = selector[1:-1].split("=")
                    if len(attr) == 2:
                        attr_name = attr[0]
                        attr_value = attr[1].strip("'\"")
                        
                        for form in structure.get("forms", []):
                            for field in form.get("fields", []):
                                aria_attrs = field.get("aria_attributes", {})
                                if aria_attrs.get(attr_name) == attr_value:
                                    elements.append(field)
                elif selector.startswith("."):
                    class_name = selector[1:]
                    
                    for form in structure.get("forms", []):
                        for field in form.get("fields", []):
                            if class_name in field.get("classes", []):
                                elements.append(field)
                
                for element in elements:
                    status_messages.append({
                        "url": url,
                        "selector": selector,
                        "element": element
                    })
                    
        return status_messages

    def _generate_summary(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generiert eine Zusammenfassung der Barrierefreiheitsanalyse"""
        summary = {
            "urls_analyzed": len(crawler_data.get("data", {})),
            "violations_count": 0,
            "warnings_count": 0,
            "passed_count": 0,
            "top_issues": [],
            "positive_aspects": []
        }
        
        # Zähle Verstöße, Warnungen und bestandene Tests
        if accessibility_results:
            summary["violations_count"] = len(accessibility_results.get("violations", []))
            summary["warnings_count"] = len(accessibility_results.get("warnings", []))
            summary["passed_count"] = len(accessibility_results.get("passed", []))
            
            # Gruppiere Verstöße nach Typ
            violation_types = {}
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                if v_type not in violation_types:
                    violation_types[v_type] = 0
                violation_types[v_type] += 1
                
            # Top-Probleme
            top_issues = sorted(violation_types.items(), key=lambda x: x[1], reverse=True)
            summary["top_issues"] = [{"type": t, "count": c} for t, c in top_issues[:5]]
            
            # Positive Aspekte
            passed_types = {}
            for p in accessibility_results.get("passed", []):
                p_type = p.get("type", "unknown")
                if p_type not in passed_types:
                    passed_types[p_type] = 0
                passed_types[p_type] += 1
                
            top_passed = sorted(passed_types.items(), key=lambda x: x[1], reverse=True)
            summary["positive_aspects"] = [{"type": t, "count": c} for t, c in top_passed[:5]]
            
        return summary

    def _calculate_scores(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, float]:
        """Berechnet Barrierefreiheits-Scores für verschiedene Kategorien"""
        scores = {
            "perceivable": 0.0,
            "operable": 0.0,
            "understandable": 0.0,
            "robust": 0.0,
            "overall": 0.0
        }
        
        # Gewichtung nach Schweregrad
        violation_weight = 1.0
        warning_weight = 0.5
        
        # Kategorisiere Verstöße und Warnungen
        if accessibility_results:
            categories = {
                "perceivable": ["missing_alt", "color_contrast", "text_spacing"],
                "operable": ["keyboard_navigation", "skip_links", "focus_not_visible", "non_descriptive_link"],
                "understandable": ["language_missing", "form_label", "form_validation"],
                "robust": ["invalid_aria_role", "heading_hierarchy", "no_landmarks"]
            }
            
            # Zähle Verstöße pro Kategorie
            category_violations = {k: 0 for k in scores.keys()}
            category_warnings = {k: 0 for k in scores.keys()}
            
            # Verstöße
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                for category, types in categories.items():
                    if v_type in types:
                        category_violations[category] += 1
                        break
            
            # Warnungen
            for w in accessibility_results.get("warnings", []):
                w_type = w.get("type", "unknown")
                for category, types in categories.items():
                    if w_type in types:
                        category_warnings[category] += 1
                        break
            
            # Berechne Score pro Kategorie (0-100, höher ist besser)
            max_issues = 10  # Annahme: Bei mehr als 10 Verstößen ist der Score 0
            
            for category in scores.keys():
                if category != "overall":
                    weighted_issues = (category_violations[category] * violation_weight + 
                                      category_warnings[category] * warning_weight)
                    
                    # Score-Berechnung: 100 - (gewichtete Probleme / max_issues * 100)
                    # Begrenzt auf 0-100
                    raw_score = 100 - (weighted_issues / max_issues * 100)
                    scores[category] = max(0, min(100, raw_score))
            
            # Gesamtscore: Durchschnitt der Kategorien
            scores["overall"] = sum(v for k, v in scores.items() if k != "overall") / 4
            
        return scores

    def _collect_text_alternatives(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Text-Alternativen für nicht-textuelle Inhalte"""
        alternatives_data = {
            "images_without_alt": [],
            "decorative_images": [],
            "complex_images": [],
            "svg_elements": [],
            "canvas_elements": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in alternatives_data.keys():
                if key in text_alts:
                    for item in text_alts[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        alternatives_data[key].append(item_copy)
                        
        return alternatives_data

    def _collect_icons_with_text(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Icons mit Text"""
        icons_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "icons_with_text" in text_alts:
                for icon in text_alts["icons_with_text"]:
                    icon_copy = icon.copy()
                    icon_copy["url"] = url
                    icons_data.append(icon_copy)
                    
        return icons_data

    def _collect_svg_elements(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über SVG-Elemente"""
        svg_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "svg_elements" in text_alts:
                for svg in text_alts["svg_elements"]:
                    svg_copy = svg.copy()
                    svg_copy["url"] = url
                    svg_data.append(svg_copy)
                    
        return svg_data

    def _collect_landmarks(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Landmarks"""
        landmarks_data = {
            "navigation": [],
            "main": [],
            "complementary": [],
            "banner": [],
            "contentinfo": [],
            "search": [],
            "form": [],
            "region": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            landmarks = structure.get("landmarks", [])
            
            for landmark in landmarks:
                role = landmark.get("role")
                if role in landmarks_data:
                    landmark_copy = landmark.copy()
                    landmark_copy["url"] = url
                    landmarks_data[role].append(landmark_copy)
                    
        return landmarks_data

    def _collect_detailed_form_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt detaillierte Informationen über Formularfeld-Labels"""
        form_labels = {
            "with_label": [],
            "with_aria_label": [],
            "with_placeholder": [],
            "without_label": [],
            "grouped_fields": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    field_data = {
                        "form_id": form.get("id"),
                        "field_id": field.get("id"),
                        "field_type": field.get("type"),
                        "url": url
                    }
                    
                    # Kategorisiere nach Labeltyp
                    if field.get("label"):
                        field_data["label"] = field["label"]
                        form_labels["with_label"].append(field_data)
                    elif field.get("aria_attributes", {}).get("aria-label"):
                        field_data["aria_label"] = field["aria_attributes"]["aria-label"]
                        form_labels["with_aria_label"].append(field_data)
                    elif field.get("placeholder"):
                        field_data["placeholder"] = field["placeholder"]
                        form_labels["with_placeholder"].append(field_data)
                    else:
                        form_labels["without_label"].append(field_data)
                        
                    # Gruppierte Felder
                    if field.get("grouped"):
                        group_data = field_data.copy()
                        group_data["group_info"] = field["grouped"]
                        form_labels["grouped_fields"].append(group_data)
                        
        return form_labels

    def _collect_text_spacing(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textabstände"""
        spacing_data = {
            "line_height": [],
            "letter_spacing": [],
            "word_spacing": [],
            "text_indent": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styling = page_data.get("styling", {})
            text_spacing = styling.get("text_spacing", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in spacing_data.keys():
                if key in text_spacing:
                    for value in text_spacing[key]:
                        spacing_data[key].append({
                            "value": value,
                            "url": url
                        })
                        
        return spacing_data

    def _collect_keyboard_traps(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über potenzielle Keyboard-Traps"""
        trap_data = {
            "custom_widgets": [],
            "modal_dialogs": [],
            "dropdown_menus": [],
            "negative_tabindex": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            keyboard_navigation = accessibility.get("keyboard_navigation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in trap_data.keys():
                if key in keyboard_navigation:
                    for item in keyboard_navigation[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        trap_data[key].append(item_copy)
                        
        return trap_data

    def _collect_form_validation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Formularvalidierung"""
        validation_data = {
            "html5_validation": [],
            "custom_validation": [],
            "pattern_validation": [],
            "constraint_validation": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            form_validation = accessibility.get("form_validation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in validation_data.keys():
                if key in form_validation:
                    # Für Listen von Objekten
                    if isinstance(form_validation[key], list):
                        for item in form_validation[key]:
                            item_copy = item.copy()
                            item_copy["url"] = url
                            validation_data[key].append(item_copy)
                    # Für einzelne Objekte
                    elif isinstance(form_validation[key], dict):
                        item_copy = form_validation[key].copy()
                        item_copy["url"] = url
                        validation_data[key].append(item_copy)
                        
        return validation_data

    def _collect_aria_labelledby(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-labelledby-Attribute"""
        labelledby_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-labelledby" in aria_attributes:
                for item in aria_attributes["aria-labelledby"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    labelledby_data.append(item_copy)
                    
        return labelledby_data

    def _collect_aria_errormessage(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-errormessage-Attribute"""
        errormessage_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-errormessage" in aria_attributes:
                for item in aria_attributes["aria-errormessage"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    errormessage_data.append(item_copy)
                    
        return errormessage_data

    def _collect_form_hints(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Formularfeld-Hilfetexte"""
        hint_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    if field.get("help_text"):
                        hint_data.append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "field_type": field.get("type"),
                            "help_text": field.get("help_text")
                        })
                        
        return hint_data

    def _collect_doctype(self, crawler_data: Dict[str, Any]) -> Dict[str, str]:
        """Sammelt Informationen über Doctype-Deklarationen"""
        doctype_data = {}
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            doctype = structure.get("doctype")
            
            if doctype:
                doctype_data[url] = doctype
                
        return doctype_data

    def _collect_aria_live(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-live-Regionen"""
        live_regions = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-live" in aria_attributes:
                for item in aria_attributes["aria-live"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    live_regions.append(item_copy)
                    
        return live_regions

    def _collect_role_alert(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Elemente mit role='alert'"""
        alerts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_roles = accessibility.get("aria_roles", [])
            
            for role in aria_roles:
                if role.get("role") == "alert":
                    role_copy = role.copy()
                    role_copy["url"] = url
                    alerts.append(role_copy)
                    
        return alerts

    def _collect_keyboard_shortcuts(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Tastaturkürzel"""
        shortcuts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            for event in events:
                if event.get("type", "").startswith("key"):
                    shortcuts.append({
                        "url": url,
                        "event_type": event.get("type"),
                        "element": event.get("element"),
                        "id": event.get("id")
                    })
                    
        return shortcuts

    def _collect_time_limits(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeitbeschränkungen"""
        time_limits = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach setTimeout und setInterval
            for script in scripting.get("scripts", []):
                content = str(script.get("content", ""))
                
                if "setTimeout" in content or "setInterval" in content:
                    time_limits.append({
                        "url": url,
                        "type": "timeout_interval",
                        "script_id": script.get("id"),
                        "has_settimeout": "setTimeout" in content,
                        "has_setinterval": "setInterval" in content
                    })
                    
        return time_limits

    def _collect_pause_features(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Pause-Funktionen"""
        pause_features = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach Pause-Buttons in Multimedia-Elementen
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "video_controls",
                        "element_id": video.get("id"),
                        "has_controls": True
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "audio_controls",
                        "element_id": audio.get("id"),
                        "has_controls": True
                    })
                    
            # Suche nach Pause-Buttons für Animationen
            animations = page_data.get("scripting", {}).get("animations", [])
            for animation in animations:
                if animation.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "animation_controls",
                        "element_id": animation.get("id"),
                        "has_controls": True
                    })
                    
        return pause_features

    def _collect_motion_animation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Bewegung und Animation"""
        motion_data = {
            "animations": [],
            "transitions": [],
            "autoplay_media": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            structure = page_data.get("structure", {})
            
            # CSS-Animationen
            css = page_data.get("styling", {}).get("css_analysis", {})
            
            for rule in css.get("rules", []):
                declarations = rule.get("declarations", {})
                
                if "animation" in declarations or "animation-name" in declarations:
                    motion_data["animations"].append({
                        "url": url,
                        "type": "css_animation",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("animation")}
                    })
                    
                if "transition" in declarations or "transition-property" in declarations:
                    motion_data["transitions"].append({
                        "url": url,
                        "type": "css_transition",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("transition")}
                    })
                    
            # Autoplay-Medien
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_video",
                        "element_id": video.get("id")
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_audio",
                        "element_id": audio.get("id")
                    })
                    
        return motion_data

    def _collect_flashing_content(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über blinkende Inhalte"""
        flashing_content = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Animationen
            animations = scripting.get("animations", [])
            for animation in animations:
                if animation.get("frequency") and animation.get("frequency") > 2:
                    flashing_content.append({
                        "url": url,
                        "type": "animation",
                        "element_id": animation.get("id"),
                        "frequency": animation.get("frequency")
                    })
                    
            # GIF-Bilder
            structure = page_data.get("structure", {})
            for img in structure.get("images", []):
                if img.get("src") and img.get("src").lower().endswith(".gif"):
                    flashing_content.append({
                        "url": url,
                        "type": "gif_image",
                        "src": img.get("src")
                    })
                    
        return flashing_content

    def _collect_pointer_gestures(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeigergesten"""
        gestures = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die spezifische Gesten erfordern könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type in ["mousedown", "mouseup", "mousemove", "dragstart", "drag", "drop"]:
                    gestures.append({
                        "url": url,
                        "type": "mouse_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                
                if event_type in ["touchstart", "touchmove", "touchend", "swipe"]:
                    gestures.append({
                        "url": url,
                        "type": "touch_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                    
        return gestures

    def _collect_pointer_cancellation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeiger-Abbruchmöglichkeiten"""
        cancellation_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die potenziell problematisch sein könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type == "mousedown" and not any(e.get("type") == "mouseup" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "mousedown_without_mouseup",
                        "element": event.get("element")
                    })
                    
                if event_type == "touchstart" and not any(e.get("type") == "touchend" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "touchstart_without_touchend",
                        "element": event.get("element")
                    })
                    
        return cancellation_data

    def _collect_input_purpose(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über den Zweck von Eingabefeldern"""
        input_purpose_data = {
            "with_autocomplete": [],
            "with_type": [],
            "with_aria_purpose": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Felder mit autocomplete-Attribut
                    if field.get("autocomplete"):
                        input_purpose_data["with_autocomplete"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "autocomplete": field.get("autocomplete")
                        })
                        
                    # Felder mit spezifischem Typ
                    if field.get("type") in ["email", "tel", "url", "number", "date", "time", "datetime-local"]:
                        input_purpose_data["with_type"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "type": field.get("type")
                        })
                        
                    # Felder mit ARIA-Attributen für den Zweck
                    aria_attrs = field.get("aria_attributes", {})
                    if "aria-autocomplete" in aria_attrs:
                        input_purpose_data["with_aria_purpose"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "aria_autocomplete": aria_attrs.get("aria-autocomplete")
                        })
                        
        return input_purpose_data

    def _collect_motion_actuation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Bewegungssteuerung"""
        motion_actuation = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach Ereignissen, die auf Bewegungssteuerung hindeuten könnten
            events = scripting.get("event_handlers", [])
            for event in events:
                if event.get("type") in ["devicemotion", "deviceorientation", "shake", "tilt"]:
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_event",
                        "event_type": event.get("type"),
                        "element": event.get("element")
                    })
                    
            # Suche nach JavaScript-APIs für Bewegung
            scripts = scripting.get("scripts", [])
            for script in scripts:
                content = str(script.get("content", ""))
                if any(api in content for api in ["DeviceMotionEvent", "DeviceOrientationEvent", "Gyroscope", "Accelerometer"]):
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_api",
                        "script_id": script.get("id"),
                        "content_snippet": content[:100] if len(content) > 100 else content
                    })
                    
        return motion_actuation

    def _collect_status_messages(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Statusmeldungen"""
        status_messages = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach typischen Status-Elementen
            selectors = [
                "[role='status']", 
                "[role='alert']", 
                "[aria-live]", 
                ".status", 
                ".message", 
                ".notification"
            ]
            
            for selector in selectors:
                elements = []
                
                # Simuliere CSS-Selektor-Auswahl
                if selector.startswith("["):
                    attr = selector[1:-1].split("=")
                    if len(attr) == 2:
                        attr_name = attr[0]
                        attr_value = attr[1].strip("'\"")
                        
                        for form in structure.get("forms", []):
                            for field in form.get("fields", []):
                                aria_attrs = field.get("aria_attributes", {})
                                if aria_attrs.get(attr_name) == attr_value:
                                    elements.append(field)
                elif selector.startswith("."):
                    class_name = selector[1:]
                    
                    for form in structure.get("forms", []):
                        for field in form.get("fields", []):
                            if class_name in field.get("classes", []):
                                elements.append(field)
                
                for element in elements:
                    status_messages.append({
                        "url": url,
                        "selector": selector,
                        "element": element
                    })
                    
        return status_messages

    def _generate_summary(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generiert eine Zusammenfassung der Barrierefreiheitsanalyse"""
        summary = {
            "urls_analyzed": len(crawler_data.get("data", {})),
            "violations_count": 0,
            "warnings_count": 0,
            "passed_count": 0,
            "top_issues": [],
            "positive_aspects": []
        }
        
        # Zähle Verstöße, Warnungen und bestandene Tests
        if accessibility_results:
            summary["violations_count"] = len(accessibility_results.get("violations", []))
            summary["warnings_count"] = len(accessibility_results.get("warnings", []))
            summary["passed_count"] = len(accessibility_results.get("passed", []))
            
            # Gruppiere Verstöße nach Typ
            violation_types = {}
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                if v_type not in violation_types:
                    violation_types[v_type] = 0
                violation_types[v_type] += 1
                
            # Top-Probleme
            top_issues = sorted(violation_types.items(), key=lambda x: x[1], reverse=True)
            summary["top_issues"] = [{"type": t, "count": c} for t, c in top_issues[:5]]
            
            # Positive Aspekte
            passed_types = {}
            for p in accessibility_results.get("passed", []):
                p_type = p.get("type", "unknown")
                if p_type not in passed_types:
                    passed_types[p_type] = 0
                passed_types[p_type] += 1
                
            top_passed = sorted(passed_types.items(), key=lambda x: x[1], reverse=True)
            summary["positive_aspects"] = [{"type": t, "count": c} for t, c in top_passed[:5]]
            
        return summary

    def _calculate_scores(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, float]:
        """Berechnet Barrierefreiheits-Scores für verschiedene Kategorien"""
        scores = {
            "perceivable": 0.0,
            "operable": 0.0,
            "understandable": 0.0,
            "robust": 0.0,
            "overall": 0.0
        }
        
        # Gewichtung nach Schweregrad
        violation_weight = 1.0
        warning_weight = 0.5
        
        # Kategorisiere Verstöße und Warnungen
        if accessibility_results:
            categories = {
                "perceivable": ["missing_alt", "color_contrast", "text_spacing"],
                "operable": ["keyboard_navigation", "skip_links", "focus_not_visible", "non_descriptive_link"],
                "understandable": ["language_missing", "form_label", "form_validation"],
                "robust": ["invalid_aria_role", "heading_hierarchy", "no_landmarks"]
            }
            
            # Zähle Verstöße pro Kategorie
            category_violations = {k: 0 for k in scores.keys()}
            category_warnings = {k: 0 for k in scores.keys()}
            
            # Verstöße
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                for category, types in categories.items():
                    if v_type in types:
                        category_violations[category] += 1
                        break
            
            # Warnungen
            for w in accessibility_results.get("warnings", []):
                w_type = w.get("type", "unknown")
                for category, types in categories.items():
                    if w_type in types:
                        category_warnings[category] += 1
                        break
            
            # Berechne Score pro Kategorie (0-100, höher ist besser)
            max_issues = 10  # Annahme: Bei mehr als 10 Verstößen ist der Score 0
            
            for category in scores.keys():
                if category != "overall":
                    weighted_issues = (category_violations[category] * violation_weight + 
                                      category_warnings[category] * warning_weight)
                    
                    # Score-Berechnung: 100 - (gewichtete Probleme / max_issues * 100)
                    # Begrenzt auf 0-100
                    raw_score = 100 - (weighted_issues / max_issues * 100)
                    scores[category] = max(0, min(100, raw_score))
            
            # Gesamtscore: Durchschnitt der Kategorien
            scores["overall"] = sum(v for k, v in scores.items() if k != "overall") / 4
            
        return scores

    def _collect_text_alternatives(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Text-Alternativen für nicht-textuelle Inhalte"""
        alternatives_data = {
            "images_without_alt": [],
            "decorative_images": [],
            "complex_images": [],
            "svg_elements": [],
            "canvas_elements": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in alternatives_data.keys():
                if key in text_alts:
                    for item in text_alts[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        alternatives_data[key].append(item_copy)
                        
        return alternatives_data

    def _collect_icons_with_text(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Icons mit Text"""
        icons_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "icons_with_text" in text_alts:
                for icon in text_alts["icons_with_text"]:
                    icon_copy = icon.copy()
                    icon_copy["url"] = url
                    icons_data.append(icon_copy)
                    
        return icons_data

    def _collect_svg_elements(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über SVG-Elemente"""
        svg_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "svg_elements" in text_alts:
                for svg in text_alts["svg_elements"]:
                    svg_copy = svg.copy()
                    svg_copy["url"] = url
                    svg_data.append(svg_copy)
                    
        return svg_data

    def _collect_landmarks(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Landmarks"""
        landmarks_data = {
            "navigation": [],
            "main": [],
            "complementary": [],
            "banner": [],
            "contentinfo": [],
            "search": [],
            "form": [],
            "region": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            landmarks = structure.get("landmarks", [])
            
            for landmark in landmarks:
                role = landmark.get("role")
                if role in landmarks_data:
                    landmark_copy = landmark.copy()
                    landmark_copy["url"] = url
                    landmarks_data[role].append(landmark_copy)
                    
        return landmarks_data

    def _collect_detailed_form_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt detaillierte Informationen über Formularfeld-Labels"""
        form_labels = {
            "with_label": [],
            "with_aria_label": [],
            "with_placeholder": [],
            "without_label": [],
            "grouped_fields": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    field_data = {
                        "form_id": form.get("id"),
                        "field_id": field.get("id"),
                        "field_type": field.get("type"),
                        "url": url
                    }
                    
                    # Kategorisiere nach Labeltyp
                    if field.get("label"):
                        field_data["label"] = field["label"]
                        form_labels["with_label"].append(field_data)
                    elif field.get("aria_attributes", {}).get("aria-label"):
                        field_data["aria_label"] = field["aria_attributes"]["aria-label"]
                        form_labels["with_aria_label"].append(field_data)
                    elif field.get("placeholder"):
                        field_data["placeholder"] = field["placeholder"]
                        form_labels["with_placeholder"].append(field_data)
                    else:
                        form_labels["without_label"].append(field_data)
                        
                    # Gruppierte Felder
                    if field.get("grouped"):
                        group_data = field_data.copy()
                        group_data["group_info"] = field["grouped"]
                        form_labels["grouped_fields"].append(group_data)
                        
        return form_labels

    def _collect_text_spacing(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textabstände"""
        spacing_data = {
            "line_height": [],
            "letter_spacing": [],
            "word_spacing": [],
            "text_indent": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styling = page_data.get("styling", {})
            text_spacing = styling.get("text_spacing", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in spacing_data.keys():
                if key in text_spacing:
                    for value in text_spacing[key]:
                        spacing_data[key].append({
                            "value": value,
                            "url": url
                        })
                        
        return spacing_data

    def _collect_keyboard_traps(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über potenzielle Keyboard-Traps"""
        trap_data = {
            "custom_widgets": [],
            "modal_dialogs": [],
            "dropdown_menus": [],
            "negative_tabindex": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            keyboard_navigation = accessibility.get("keyboard_navigation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in trap_data.keys():
                if key in keyboard_navigation:
                    for item in keyboard_navigation[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        trap_data[key].append(item_copy)
                        
        return trap_data

    def _collect_form_validation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Formularvalidierung"""
        validation_data = {
            "html5_validation": [],
            "custom_validation": [],
            "pattern_validation": [],
            "constraint_validation": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            form_validation = accessibility.get("form_validation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in validation_data.keys():
                if key in form_validation:
                    # Für Listen von Objekten
                    if isinstance(form_validation[key], list):
                        for item in form_validation[key]:
                            item_copy = item.copy()
                            item_copy["url"] = url
                            validation_data[key].append(item_copy)
                    # Für einzelne Objekte
                    elif isinstance(form_validation[key], dict):
                        item_copy = form_validation[key].copy()
                        item_copy["url"] = url
                        validation_data[key].append(item_copy)
                        
        return validation_data

    def _collect_aria_labelledby(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-labelledby-Attribute"""
        labelledby_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-labelledby" in aria_attributes:
                for item in aria_attributes["aria-labelledby"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    labelledby_data.append(item_copy)
                    
        return labelledby_data

    def _collect_aria_errormessage(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-errormessage-Attribute"""
        errormessage_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-errormessage" in aria_attributes:
                for item in aria_attributes["aria-errormessage"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    errormessage_data.append(item_copy)
                    
        return errormessage_data

    def _collect_form_hints(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Formularfeld-Hilfetexte"""
        hint_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    if field.get("help_text"):
                        hint_data.append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "field_type": field.get("type"),
                            "help_text": field.get("help_text")
                        })
                        
        return hint_data

    def _collect_doctype(self, crawler_data: Dict[str, Any]) -> Dict[str, str]:
        """Sammelt Informationen über Doctype-Deklarationen"""
        doctype_data = {}
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            doctype = structure.get("doctype")
            
            if doctype:
                doctype_data[url] = doctype
                
        return doctype_data

    def _collect_aria_live(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über aria-live-Regionen"""
        live_regions = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_attributes = accessibility.get("aria_attributes", {})
            
            if "aria-live" in aria_attributes:
                for item in aria_attributes["aria-live"]:
                    item_copy = item.copy()
                    item_copy["url"] = url
                    live_regions.append(item_copy)
                    
        return live_regions

    def _collect_role_alert(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Elemente mit role='alert'"""
        alerts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            aria_roles = accessibility.get("aria_roles", [])
            
            for role in aria_roles:
                if role.get("role") == "alert":
                    role_copy = role.copy()
                    role_copy["url"] = url
                    alerts.append(role_copy)
                    
        return alerts

    def _collect_keyboard_shortcuts(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Tastaturkürzel"""
        shortcuts = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            for event in events:
                if event.get("type", "").startswith("key"):
                    shortcuts.append({
                        "url": url,
                        "event_type": event.get("type"),
                        "element": event.get("element"),
                        "id": event.get("id")
                    })
                    
        return shortcuts

    def _collect_time_limits(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeitbeschränkungen"""
        time_limits = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach setTimeout und setInterval
            for script in scripting.get("scripts", []):
                content = str(script.get("content", ""))
                
                if "setTimeout" in content or "setInterval" in content:
                    time_limits.append({
                        "url": url,
                        "type": "timeout_interval",
                        "script_id": script.get("id"),
                        "has_settimeout": "setTimeout" in content,
                        "has_setinterval": "setInterval" in content
                    })
                    
        return time_limits

    def _collect_pause_features(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Pause-Funktionen"""
        pause_features = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach Pause-Buttons in Multimedia-Elementen
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "video_controls",
                        "element_id": video.get("id"),
                        "has_controls": True
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "audio_controls",
                        "element_id": audio.get("id"),
                        "has_controls": True
                    })
                    
            # Suche nach Pause-Buttons für Animationen
            animations = page_data.get("scripting", {}).get("animations", [])
            for animation in animations:
                if animation.get("controls"):
                    pause_features.append({
                        "url": url,
                        "type": "animation_controls",
                        "element_id": animation.get("id"),
                        "has_controls": True
                    })
                    
        return pause_features

    def _collect_motion_animation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Bewegung und Animation"""
        motion_data = {
            "animations": [],
            "transitions": [],
            "autoplay_media": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            structure = page_data.get("structure", {})
            
            # CSS-Animationen
            css = page_data.get("styling", {}).get("css_analysis", {})
            
            for rule in css.get("rules", []):
                declarations = rule.get("declarations", {})
                
                if "animation" in declarations or "animation-name" in declarations:
                    motion_data["animations"].append({
                        "url": url,
                        "type": "css_animation",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("animation")}
                    })
                    
                if "transition" in declarations or "transition-property" in declarations:
                    motion_data["transitions"].append({
                        "url": url,
                        "type": "css_transition",
                        "selector": rule.get("selector"),
                        "properties": {k: v for k, v in declarations.items() if k.startswith("transition")}
                    })
                    
            # Autoplay-Medien
            multimedia = structure.get("multimedia", {})
            
            for video in multimedia.get("videos", []):
                if video.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_video",
                        "element_id": video.get("id")
                    })
                    
            for audio in multimedia.get("audio", []):
                if audio.get("autoplay"):
                    motion_data["autoplay_media"].append({
                        "url": url,
                        "type": "autoplay_audio",
                        "element_id": audio.get("id")
                    })
                    
        return motion_data

    def _collect_flashing_content(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über blinkende Inhalte"""
        flashing_content = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Animationen
            animations = scripting.get("animations", [])
            for animation in animations:
                if animation.get("frequency") and animation.get("frequency") > 2:
                    flashing_content.append({
                        "url": url,
                        "type": "animation",
                        "element_id": animation.get("id"),
                        "frequency": animation.get("frequency")
                    })
                    
            # GIF-Bilder
            structure = page_data.get("structure", {})
            for img in structure.get("images", []):
                if img.get("src") and img.get("src").lower().endswith(".gif"):
                    flashing_content.append({
                        "url": url,
                        "type": "gif_image",
                        "src": img.get("src")
                    })
                    
        return flashing_content

    def _collect_pointer_gestures(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeigergesten"""
        gestures = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die spezifische Gesten erfordern könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type in ["mousedown", "mouseup", "mousemove", "dragstart", "drag", "drop"]:
                    gestures.append({
                        "url": url,
                        "type": "mouse_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                
                if event_type in ["touchstart", "touchmove", "touchend", "swipe"]:
                    gestures.append({
                        "url": url,
                        "type": "touch_gesture",
                        "event_type": event_type,
                        "element": event.get("element")
                    })
                    
        return gestures

    def _collect_pointer_cancellation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Zeiger-Abbruchmöglichkeiten"""
        cancellation_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            events = scripting.get("event_handlers", [])
            
            # Suche nach Ereignissen, die potenziell problematisch sein könnten
            for event in events:
                event_type = event.get("type", "")
                
                if event_type == "mousedown" and not any(e.get("type") == "mouseup" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "mousedown_without_mouseup",
                        "element": event.get("element")
                    })
                    
                if event_type == "touchstart" and not any(e.get("type") == "touchend" for e in events):
                    cancellation_data.append({
                        "url": url,
                        "type": "touchstart_without_touchend",
                        "element": event.get("element")
                    })
                    
        return cancellation_data

    def _collect_input_purpose(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über den Zweck von Eingabefeldern"""
        input_purpose_data = {
            "with_autocomplete": [],
            "with_type": [],
            "with_aria_purpose": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Felder mit autocomplete-Attribut
                    if field.get("autocomplete"):
                        input_purpose_data["with_autocomplete"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "autocomplete": field.get("autocomplete")
                        })
                        
                    # Felder mit spezifischem Typ
                    if field.get("type") in ["email", "tel", "url", "number", "date", "time", "datetime-local"]:
                        input_purpose_data["with_type"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "type": field.get("type")
                        })
                        
                    # Felder mit ARIA-Attributen für den Zweck
                    aria_attrs = field.get("aria_attributes", {})
                    if "aria-autocomplete" in aria_attrs:
                        input_purpose_data["with_aria_purpose"].append({
                            "url": url,
                            "form_id": form.get("id"),
                            "field_id": field.get("id"),
                            "aria_autocomplete": aria_attrs.get("aria-autocomplete")
                        })
                        
        return input_purpose_data

    def _collect_motion_actuation(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Bewegungssteuerung"""
        motion_actuation = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            scripting = page_data.get("scripting", {})
            
            # Suche nach Ereignissen, die auf Bewegungssteuerung hindeuten könnten
            events = scripting.get("event_handlers", [])
            for event in events:
                if event.get("type") in ["devicemotion", "deviceorientation", "shake", "tilt"]:
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_event",
                        "event_type": event.get("type"),
                        "element": event.get("element")
                    })
                    
            # Suche nach JavaScript-APIs für Bewegung
            scripts = scripting.get("scripts", [])
            for script in scripts:
                content = str(script.get("content", ""))
                if any(api in content for api in ["DeviceMotionEvent", "DeviceOrientationEvent", "Gyroscope", "Accelerometer"]):
                    motion_actuation.append({
                        "url": url,
                        "type": "motion_api",
                        "script_id": script.get("id"),
                        "content_snippet": content[:100] if len(content) > 100 else content
                    })
                    
        return motion_actuation

    def _collect_status_messages(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Statusmeldungen"""
        status_messages = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            
            # Suche nach typischen Status-Elementen
            selectors = [
                "[role='status']", 
                "[role='alert']", 
                "[aria-live]", 
                ".status", 
                ".message", 
                ".notification"
            ]
            
            for selector in selectors:
                elements = []
                
                # Simuliere CSS-Selektor-Auswahl
                if selector.startswith("["):
                    attr = selector[1:-1].split("=")
                    if len(attr) == 2:
                        attr_name = attr[0]
                        attr_value = attr[1].strip("'\"")
                        
                        for form in structure.get("forms", []):
                            for field in form.get("fields", []):
                                aria_attrs = field.get("aria_attributes", {})
                                if aria_attrs.get(attr_name) == attr_value:
                                    elements.append(field)
                elif selector.startswith("."):
                    class_name = selector[1:]
                    
                    for form in structure.get("forms", []):
                        for field in form.get("fields", []):
                            if class_name in field.get("classes", []):
                                elements.append(field)
                
                for element in elements:
                    status_messages.append({
                        "url": url,
                        "selector": selector,
                        "element": element
                    })
                    
        return status_messages

    def _generate_summary(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generiert eine Zusammenfassung der Barrierefreiheitsanalyse"""
        summary = {
            "urls_analyzed": len(crawler_data.get("data", {})),
            "violations_count": 0,
            "warnings_count": 0,
            "passed_count": 0,
            "top_issues": [],
            "positive_aspects": []
        }
        
        # Zähle Verstöße, Warnungen und bestandene Tests
        if accessibility_results:
            summary["violations_count"] = len(accessibility_results.get("violations", []))
            summary["warnings_count"] = len(accessibility_results.get("warnings", []))
            summary["passed_count"] = len(accessibility_results.get("passed", []))
            
            # Gruppiere Verstöße nach Typ
            violation_types = {}
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                if v_type not in violation_types:
                    violation_types[v_type] = 0
                violation_types[v_type] += 1
                
            # Top-Probleme
            top_issues = sorted(violation_types.items(), key=lambda x: x[1], reverse=True)
            summary["top_issues"] = [{"type": t, "count": c} for t, c in top_issues[:5]]
            
            # Positive Aspekte
            passed_types = {}
            for p in accessibility_results.get("passed", []):
                p_type = p.get("type", "unknown")
                if p_type not in passed_types:
                    passed_types[p_type] = 0
                passed_types[p_type] += 1
                
            top_passed = sorted(passed_types.items(), key=lambda x: x[1], reverse=True)
            summary["positive_aspects"] = [{"type": t, "count": c} for t, c in top_passed[:5]]
            
        return summary

    def _calculate_scores(self, crawler_data: Dict[str, Any], accessibility_results: Dict[str, Any]) -> Dict[str, float]:
        """Berechnet Barrierefreiheits-Scores für verschiedene Kategorien"""
        scores = {
            "perceivable": 0.0,
            "operable": 0.0,
            "understandable": 0.0,
            "robust": 0.0,
            "overall": 0.0
        }
        
        # Gewichtung nach Schweregrad
        violation_weight = 1.0
        warning_weight = 0.5
        
        # Kategorisiere Verstöße und Warnungen
        if accessibility_results:
            categories = {
                "perceivable": ["missing_alt", "color_contrast", "text_spacing"],
                "operable": ["keyboard_navigation", "skip_links", "focus_not_visible", "non_descriptive_link"],
                "understandable": ["language_missing", "form_label", "form_validation"],
                "robust": ["invalid_aria_role", "heading_hierarchy", "no_landmarks"]
            }
            
            # Zähle Verstöße pro Kategorie
            category_violations = {k: 0 for k in scores.keys()}
            category_warnings = {k: 0 for k in scores.keys()}
            
            # Verstöße
            for v in accessibility_results.get("violations", []):
                v_type = v.get("type", "unknown")
                for category, types in categories.items():
                    if v_type in types:
                        category_violations[category] += 1
                        break
            
            # Warnungen
            for w in accessibility_results.get("warnings", []):
                w_type = w.get("type", "unknown")
                for category, types in categories.items():
                    if w_type in types:
                        category_warnings[category] += 1
                        break
            
            # Berechne Score pro Kategorie (0-100, höher ist besser)
            max_issues = 10  # Annahme: Bei mehr als 10 Verstößen ist der Score 0
            
            for category in scores.keys():
                if category != "overall":
                    weighted_issues = (category_violations[category] * violation_weight + 
                                      category_warnings[category] * warning_weight)
                    
                    # Score-Berechnung: 100 - (gewichtete Probleme / max_issues * 100)
                    # Begrenzt auf 0-100
                    raw_score = 100 - (weighted_issues / max_issues * 100)
                    scores[category] = max(0, min(100, raw_score))
            
            # Gesamtscore: Durchschnitt der Kategorien
            scores["overall"] = sum(v for k, v in scores.items() if k != "overall") / 4
            
        return scores

    def _collect_language_declaration(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Sprachdeklarationen"""
        language_data = {
            "declared_languages": [],
            "missing_declarations": [],
            "invalid_codes": [],
            "inconsistent_declarations": []
        }
        
        valid_language_codes = ["de", "en", "fr", "es", "it"]  # Beispielliste
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe Hauptsprache
            html_lang = page_data.get("accessibility", {}).get("language", {}).get("main_language")
            
            if html_lang:
                if html_lang.lower() in valid_language_codes:
                    language_data["declared_languages"].append({
                        "url": url,
                        "language": html_lang
                    })
                else:
                    language_data["invalid_codes"].append({
                        "url": url,
                        "language": html_lang
                    })
            else:
                language_data["missing_declarations"].append({
                            "url": url
                        })
                        
            # Prüfe auf Inkonsistenzen
            meta_lang = page_data.get("metadata", {}).get("language")
            if html_lang and meta_lang and html_lang != meta_lang:
                language_data["inconsistent_declarations"].append({
                    "url": url,
                    "html_lang": html_lang,
                    "meta_lang": meta_lang
                })
        
        return language_data

    def _collect_language_changes(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Sprachwechsel"""
        changes_data = {
            "marked_changes": [],
            "unmarked_changes": [],
            "invalid_language_codes": []
        }
        
        valid_language_codes = ["de", "en", "fr", "es", "it"]  # Beispielliste
        
        for url, page_data in crawler_data.get("data", {}).items():
            main_language = page_data.get("accessibility", {}).get("language", {}).get("main_language")
            
            # Prüfe Sprachwechsel in Elementen
            for element in page_data.get("accessibility", {}).get("language", {}).get("language_elements", []):
                element_lang = element.get("lang")
                
                if element_lang and element_lang != main_language:
                    if element_lang.lower() in valid_language_codes:
                        changes_data["marked_changes"].append({
                            "url": url,
                            "element": element,
                            "from_language": main_language,
                            "to_language": element_lang
                        })
                    else:
                        changes_data["invalid_language_codes"].append({
                            "url": url,
                            "element": element,
                            "language_code": element_lang
                        })
                elif self._detect_language_change(element.get("text", ""), main_language):
                    changes_data["unmarked_changes"].append({
                        "url": url,
                        "element": element,
                        "main_language": main_language
                    })
        
        return changes_data

    def _detect_language_change(self, text: str, main_language: str) -> bool:
        """Erkennt potenzielle Sprachwechsel im Text"""
        # Vereinfachte Implementierung - in der Praxis würde hier
        # eine ausführlichere Spracherkennung stehen
        if not text or not main_language:
            return False
        
        # Beispielhafte Erkennung basierend auf Wortlisten
        german_words = {"der", "die", "das", "und", "oder", "für"}
        english_words = {"the", "and", "or", "for", "with", "from"}
        
        words = set(text.lower().split())
        
        if main_language.startswith("de"):
            return bool(words & english_words)
        elif main_language.startswith("en"):
            return bool(words & german_words)
        
        return False

    def _collect_navigation_patterns(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Navigationsmuster"""
        pattern_data = {
            "consistent_patterns": [],
            "inconsistent_patterns": [],
            "navigation_locations": [],
            "navigation_styles": []
        }
        
        navigation_patterns = {}  # Speichert Muster für Vergleiche
        
        for url, page_data in crawler_data.get("data", {}).items():
            navigation = page_data.get("structure", {}).get("navigation", [])
            
            # Erstelle Navigationsmuster
            current_pattern = {
                "structure": [nav.get("type") for nav in navigation],
                "labels": [nav.get("aria_label") for nav in navigation],
                "locations": [nav.get("position") for nav in navigation]
            }
            
            pattern_key = str(current_pattern["structure"])
            
            # Vergleiche mit vorherigen Mustern
            if pattern_key in navigation_patterns:
                if navigation_patterns[pattern_key] == current_pattern:
                    pattern_data["consistent_patterns"].append({
                        "url": url,
                        "pattern": current_pattern
                    })
                else:
                    pattern_data["inconsistent_patterns"].append({
                        "url": url,
                        "current": current_pattern,
                        "expected": navigation_patterns[pattern_key]
                    })
            else:
                navigation_patterns[pattern_key] = current_pattern
            
            # Sammle Navigationspositionen
            for nav in navigation:
                pattern_data["navigation_locations"].append({
                        "url": url,
                    "navigation": nav,
                    "position": nav.get("position")
                })
            
            # Sammle Navigationsstile
            styles = page_data.get("styling", {})
            for nav in navigation:
                nav_styles = self._get_element_styles(nav, styles)
                if nav_styles:
                    pattern_data["navigation_styles"].append({
                        "url": url,
                        "navigation": nav,
                        "styles": nav_styles
                    })
        
        return pattern_data

    def _get_element_styles(self, element: Dict[str, Any], styles: Dict[str, Any]) -> Dict[str, Any]:
        """Extrahiert Stil-Informationen für ein Element"""
        if not self._ensure_dict(element):
            return {}
            
        result = {}
        
        # Element-ID
        if element.get("id"):
            element_id = element.get("id")
            id_styles = styles.get("id_styles", {}).get(element_id, {})
            result.update(id_styles)
        
        # Element-Klassen
        for class_name in element.get("class", []):
            class_styles = styles.get("class_styles", {}).get(class_name, {})
            result.update(class_styles)
        
        # Inline-Styles
        if element.get("style"):
            inline_styles = self._parse_css_properties(element.get("style", ""))
            result.update(inline_styles)
        
        return result

    def _collect_menu_structure(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Menüstrukturen"""
        menu_data = {
            "main_menus": [],
            "sub_menus": [],
            "menu_items": [],
            "menu_states": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            navigation = page_data.get("structure", {}).get("navigation", [])
            
            for nav in navigation:
                # Hauptmenüs
                if nav.get("role") == "menubar" or "main-menu" in str(nav.get("class", "")):
                    menu_data["main_menus"].append({
                        "url": url,
                        "menu": nav
                    })
                    
                    # Untermenüs
                    for submenu in nav.get("children", []):
                        if submenu.get("role") == "menu" or "sub-menu" in str(submenu.get("class", "")):
                            menu_data["sub_menus"].append({
                        "url": url,
                                "menu": submenu,
                                "parent": nav.get("id")
                            })
                    
                    # Menüpunkte
                    for item in nav.get("children", []):
                        if item.get("role") == "menuitem":
                            menu_data["menu_items"].append({
                        "url": url,
                                "item": item,
                                "menu": nav.get("id")
                            })
                    
                    # Menüzustände
                    for item in nav.get("children", []):
                        if item.get("aria-expanded") is not None:
                            menu_data["menu_states"].append({
                        "url": url,
                                "item": item,
                                "state": item.get("aria-expanded")
                            })
        
        return menu_data

    def _collect_consistent_labeling(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über konsistente Bezeichnungen"""
        labeling_data = {
            "consistent_labels": [],
            "inconsistent_labels": [],
            "function_labels": [],
            "component_labels": []
        }
        
        label_patterns = {}  # Speichert Bezeichnungsmuster
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe Formularfelder
            for form in page_data.get("structure", {}).get("forms", []):
                for field in form.get("fields", []):
                    field_type = field.get("type")
                    field_label = field.get("label") or field.get("aria_label")
                    
                    if field_type and field_label:
                        pattern_key = f"{field_type}_label"
                        
                        if pattern_key in label_patterns:
                            if label_patterns[pattern_key] == field_label:
                                labeling_data["consistent_labels"].append({
                        "url": url,
                                    "field": field,
                                    "label": field_label
                    })
                            else:
                                labeling_data["inconsistent_labels"].append({
                        "url": url,
                                    "field": field,
                                    "current_label": field_label,
                                    "expected_label": label_patterns[pattern_key]
                                })
                        else:
                            label_patterns[pattern_key] = field_label
            
            # Prüfe Funktionsbezeichnungen
            for button in page_data.get("structure", {}).get("interactive_elements", {}).get("buttons", []):
                function_name = button.get("text") or button.get("aria_label")
                if function_name:
                    labeling_data["function_labels"].append({
                        "url": url,
                        "button": button,
                        "label": function_name
                    })
            
            # Prüfe Komponentenbezeichnungen
            for component in page_data.get("structure", {}).get("interactive_elements", {}).get("custom_controls", []):
                component_name = component.get("aria_label")
                if component_name:
                    labeling_data["component_labels"].append({
                        "url": url,
                        "component": component,
                        "label": component_name
                    })
        
        return labeling_data

    def _collect_error_identification(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Fehlererkennung"""
        error_data = {
            "form_validation": [],
            "error_messages": [],
            "error_indicators": [],
            "error_prevention": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            forms = page_data.get("structure", {}).get("forms", [])
            
            for form in forms:
                # Formularvalidierung
                validation = self._analyze_form_validation(form)
                if validation:
                    error_data["form_validation"].append({
                            "url": url,
                        "form": form,
                        "validation": validation
                    })
                
                # Fehlermeldungen
                for field in form.get("fields", []):
                    error_message = field.get("aria-errormessage") or field.get("data-error")
                    if error_message:
                        error_data["error_messages"].append({
                            "url": url,
                            "field": field,
                            "message": error_message
                        })
                
                # Fehlerindikatoren
                for field in form.get("fields", []):
                    if field.get("aria-invalid") is not None:
                        error_data["error_indicators"].append({
                            "url": url,
                            "field": field,
                            "state": field.get("aria-invalid")
                        })
                
                # Fehlerprävention
                prevention = self._analyze_error_prevention(form)
                if prevention:
                    error_data["error_prevention"].append({
                        "url": url,
                        "form": form,
                        "prevention": prevention
                    })
        
        return error_data

    def _analyze_form_validation(self, form: Dict[str, Any]) -> Dict[str, Any]:
        """Analysiert die Formularvalidierung"""
        validation = {
            "client_side": False,
            "required_fields": [],
            "pattern_validation": [],
            "custom_validation": []
        }
        
        # Prüfe clientseitige Validierung
        if form.get("novalidate") is None:
            validation["client_side"] = True
        
        # Sammle Pflichtfelder
                            for field in form.get("fields", []):
            if field.get("required"):
                validation["required_fields"].append({
                    "field": field,
                    "type": field.get("type")
                })
        
        # Prüfe Muster-Validierung
                        for field in form.get("fields", []):
            if field.get("pattern"):
                validation["pattern_validation"].append({
                    "field": field,
                    "pattern": field.get("pattern")
                })
        
        # Prüfe benutzerdefinierte Validierung
        scripts = form.get("scripts", [])
        for script in scripts:
            if "validate" in str(script) or "validation" in str(script):
                validation["custom_validation"].append({
                    "script": script
                })
        
        return validation

    def _analyze_error_prevention(self, form: Dict[str, Any]) -> Dict[str, Any]:
        """Analysiert die Fehlerprävention"""
        prevention = {
            "reversible": False,
            "checked": False,
            "confirmed": False
        }
        
        # Prüfe auf Umkehrbarkeit (z.B. Zurück-Button)
        if any(button.get("type") == "reset" for button in form.get("buttons", [])):
            prevention["reversible"] = True
        
        # Prüfe auf Überprüfung
        if form.get("data-confirm") or form.get("onsubmit"):
            prevention["checked"] = True
        
        # Prüfe auf Bestätigung
        if form.get("data-confirm-submit") or "confirm" in str(form.get("onsubmit", "")):
            prevention["confirmed"] = True
        
        return prevention

    def _collect_form_labels_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Formular-Labels"""
        label_data = {
            "labeled_fields": [],
            "unlabeled_fields": [],
            "implicit_labels": [],
            "aria_labels": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            forms = page_data.get("structure", {}).get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Explizite Labels
                    if field.get("label"):
                        label_data["labeled_fields"].append({
                            "url": url,
                            "field": field,
                            "label": field.get("label")
                        })
                    else:
                        label_data["unlabeled_fields"].append({
                        "url": url,
                            "field": field
                        })
                    
                    # Implizite Labels
                    if field.get("placeholder"):
                        label_data["implicit_labels"].append({
                        "url": url,
                            "field": field,
                            "placeholder": field.get("placeholder")
                        })
                    
                    # ARIA-Labels
                    if field.get("aria-label") or field.get("aria-labelledby"):
                        label_data["aria_labels"].append({
                        "url": url,
                            "field": field,
                            "aria_label": field.get("aria-label"),
                            "aria_labelledby": field.get("aria-labelledby")
                        })
        
        return label_data

    def _collect_error_descriptions(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Fehlerbeschreibungen"""
        description_data = {
            "error_messages": [],
            "error_suggestions": [],
            "error_identification": [],
            "error_locations": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            forms = page_data.get("structure", {}).get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Fehlermeldungen
                    if field.get("aria-errormessage"):
                        description_data["error_messages"].append({
                        "url": url,
                            "field": field,
                            "message": field.get("aria-errormessage")
                    })
                    
                    # Fehlervorschläge
                    if field.get("title") or field.get("data-error-hint"):
                        description_data["error_suggestions"].append({
                        "url": url,
                            "field": field,
                            "suggestion": field.get("title") or field.get("data-error-hint")
                        })
                    
                    # Fehleridentifikation
                    if field.get("aria-invalid"):
                        description_data["error_identification"].append({
                        "url": url,
                            "field": field,
                            "state": field.get("aria-invalid")
                        })
                    
                    # Fehlerposition
                    if field.get("data-error-location"):
                        description_data["error_locations"].append({
                        "url": url,
                            "field": field,
                            "location": field.get("data-error-location")
                        })
        
        return description_data

    def _collect_help_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Hilfestellungen"""
        help_data = {
            "help_text": [],
            "tooltips": [],
            "context_help": [],
            "documentation": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            forms = page_data.get("structure", {}).get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    # Hilfetext
                    if field.get("aria-describedby"):
                        help_data["help_text"].append({
                        "url": url,
                            "field": field,
                            "description": field.get("aria-describedby")
                        })
                    
                    # Tooltips
                    if field.get("title"):
                        help_data["tooltips"].append({
                        "url": url,
                            "field": field,
                            "tooltip": field.get("title")
                        })
            
            # Kontexthilfe
            for element in page_data.get("structure", {}).get("interactive_elements", {}).get("all", []):
                if element.get("role") == "tooltip" or element.get("class", "") == "help":
                    help_data["context_help"].append({
                        "url": url,
                        "element": element
                    })
            
            # Dokumentation
            for link in page_data.get("structure", {}).get("links", []):
                if "help" in str(link.get("href", "")) or "doc" in str(link.get("href", "")):
                    help_data["documentation"].append({
                        "url": url,
                        "link": link
                    })
        
        return help_data

    def _collect_html_validation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über HTML-Validierung"""
        validation_data = {
            "doctype": [],
            "character_encoding": [],
            "valid_elements": [],
            "invalid_elements": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Prüfe Doctype
            doctype = page_data.get("structure", {}).get("doctype")
            if doctype:
                validation_data["doctype"].append({
                            "url": url,
                    "doctype": doctype
                })
            
            # Prüfe Zeichenkodierung
            charset = page_data.get("metadata", {}).get("charset")
            if charset:
                validation_data["character_encoding"].append({
                            "url": url,
                    "charset": charset
                })
            
            # Prüfe HTML-Elemente
            structure = page_data.get("structure", {})
            for element_type in structure:
                elements = structure[element_type]
                if isinstance(elements, list):
                for element in elements:
                        if self._is_valid_html_element(element):
                            validation_data["valid_elements"].append({
                        "url": url,
                        "element": element
                    })
                    else:
                            validation_data["invalid_elements"].append({
                                "url": url,
                                "element": element
                            })
        
        return validation_data

    def _is_valid_html_element(self, element: Dict[str, Any]) -> bool:
        """Prüft, ob ein HTML-Element valide ist"""
        if not self._ensure_dict(element):
            return False
            
        # Liste gültiger HTML5-Elemente
        valid_elements = {
            "a", "abbr", "address", "area", "article", "aside", "audio",
            "b", "base", "bdi", "bdo", "blockquote", "body", "br", "button",
            "canvas", "caption", "cite", "code", "col", "colgroup",
            "data", "datalist", "dd", "del", "details", "dfn", "dialog", "div", "dl", "dt",
            "em", "embed",
            "fieldset", "figcaption", "figure", "footer", "form",
            "h1", "h2", "h3", "h4", "h5", "h6", "head", "header", "hr", "html",
            "i", "iframe", "img", "input", "ins",
            "kbd",
            "label", "legend", "li", "link",
            "main", "map", "mark", "meta", "meter",
            "nav", "noscript",
            "object", "ol", "optgroup", "option", "output",
            "p", "param", "picture", "pre", "progress",
            "q",
            "rp", "rt", "ruby",
            "s", "samp", "script", "section", "select", "small", "source", "span",
            "strong", "style", "sub", "summary", "sup",
            "table", "tbody", "td", "template", "textarea", "tfoot", "th",
            "thead", "time", "title", "tr", "track",
            "u", "ul",
            "var", "video",
            "wbr"
        }
        
        return element.get("name", "").lower() in valid_elements

    def _collect_css_validation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über CSS-Validierung"""
        css_data = {
            "valid_properties": [],
            "invalid_properties": [],
            "vendor_prefixes": [],
            "important_rules": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styles = page_data.get("styling", {})
            
            # Prüfe CSS-Eigenschaften
            for style in styles.get("css_analysis", {}).get("inline_styles", []):
                properties = self._parse_css_properties(style)
                
                for prop, value in properties.items():
                    if self._is_valid_css_property(prop):
                        css_data["valid_properties"].append({
                            "url": url,
                            "property": prop,
                            "value": value
                        })
                    else:
                        css_data["invalid_properties"].append({
                        "url": url,
                            "property": prop,
                            "value": value
                        })
                    
                    # Prüfe Vendor-Präfixe
                    if prop.startswith(("-webkit-", "-moz-", "-ms-", "-o-")):
                        css_data["vendor_prefixes"].append({
                        "url": url,
                            "property": prop,
                            "value": value
                        })
                    
                    # Prüfe !important
                    if "!important" in str(value):
                        css_data["important_rules"].append({
                        "url": url,
                            "property": prop,
                            "value": value
                        })
        
        return css_data

    def _parse_css_properties(self, style: str) -> Dict[str, str]:
        """Extrahiert CSS-Eigenschaften aus einem Style-String"""
        properties = {}
        if isinstance(style, str):
            # Entferne Kommentare
            style = re.sub(r'/\*.*?\*/', '', style)
            # Finde alle Eigenschaft-Wert-Paare
            matches = re.findall(r'([\w-]+)\s*:\s*([^;]+);?', style)
            for prop, value in matches:
                properties[prop.strip()] = value.strip()
        return properties

    def _is_valid_css_property(self, property_name: str) -> bool:
        """Prüft, ob eine CSS-Eigenschaft valide ist"""
        # Liste gängiger CSS-Eigenschaften
        valid_properties = {
            "align-content", "align-items", "align-self", "animation",
            "background", "background-color", "border", "border-radius",
            "color", "content", "cursor",
            "display",
            "flex", "flex-basis", "flex-direction", "flex-grow", "flex-shrink", "flex-wrap",
            "font", "font-family", "font-size", "font-weight",
            "grid", "grid-area", "grid-template-columns", "grid-template-rows",
            "height",
            "justify-content",
            "line-height",
            "margin", "max-height", "max-width", "min-height", "min-width",
            "opacity", "order", "outline",
            "padding", "position",
            "text-align", "text-decoration", "text-transform", "transform",
            "transition",
            "vertical-align", "visibility",
            "width", "z-index"
        }
        
        # Entferne Vendor-Präfixe für die Prüfung
        clean_property = re.sub(r'^-(?:webkit|moz|ms|o)-', '', property_name)
        return clean_property in valid_properties

    def _collect_aria_usage(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Nutzung"""
        aria_data = {
            "roles": [],
            "properties": [],
            "states": [],
            "labels": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            # Sammle ARIA-Rollen
            for element in page_data.get("accessibility", {}).get("aria_roles", []):
                if element.get("role"):
                    aria_data["roles"].append({
                        "url": url,
                        "element": element,
                        "role": element.get("role")
                    })
            
            # Sammle ARIA-Eigenschaften und -Zustände
            for element in page_data.get("accessibility", {}).get("aria_labels", []):
                for attr, value in element.get("aria_attrs", {}).items():
                    if attr.startswith("aria-"):
                        if self._is_aria_state(attr):
                            aria_data["states"].append({
                        "url": url,
                                "element": element,
                                "attribute": attr,
                                "value": value
                            })
                        else:
                            aria_data["properties"].append({
                        "url": url,
                                "element": element,
                                "attribute": attr,
                                "value": value
                            })
            
            # Sammle ARIA-Labels
            for element in page_data.get("accessibility", {}).get("aria_labels", []):
                if element.get("aria_attrs", {}).get("aria-label"):
                    aria_data["labels"].append({
                        "url": url,
                        "element": element,
                        "label": element.get("aria_attrs", {}).get("aria-label")
                    })
        
        return aria_data

    def _is_aria_state(self, attribute: str) -> bool:
        """Prüft, ob ein ARIA-Attribut ein Zustand ist"""
        states = {
            "aria-busy", "aria-checked", "aria-disabled", "aria-expanded",
            "aria-grabbed", "aria-hidden", "aria-invalid", "aria-pressed",
            "aria-selected"
        }
        return attribute in states

    def _collect_custom_controls_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über benutzerdefinierte Steuerelemente"""
        controls_data = {
            "widgets": [],
            "composite": [],
            "live_regions": [],
            "drag_drop": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            elements = page_data.get("structure", {}).get("interactive_elements", {})
            
            # Benutzerdefinierte Widgets
            for widget in elements.get("custom_controls", []):
                if widget.get("role") in ["button", "checkbox", "radio", "slider", "spinbutton"]:
                    controls_data["widgets"].append({
                        "url": url,
                        "widget": widget
                    })
            
            # Zusammengesetzte Widgets
            for composite in elements.get("custom_controls", []):
                if composite.get("role") in ["combobox", "grid", "listbox", "menu", "radiogroup", "tablist", "tree"]:
                    controls_data["composite"].append({
                        "url": url,
                        "composite": composite
                    })
            
            # Live-Regionen
            for region in elements.get("custom_controls", []):
                if region.get("aria-live"):
                    controls_data["live_regions"].append({
                        "url": url,
                        "region": region
                    })
            
            # Drag & Drop
            for element in elements.get("custom_controls", []):
                if element.get("aria-grabbed") is not None or element.get("aria-dropeffect"):
                    controls_data["drag_drop"].append({
                        "url": url,
                        "element": element
                    })
        
        return controls_data

    def _collect_aria_implementation(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Implementierung"""
        implementation_data = {
            "landmarks": [],
            "live_regions": [],
            "relationships": [],
            "validation": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            
            # Landmarks
            for landmark in accessibility.get("aria_roles", []):
                if landmark.get("role") in ["banner", "complementary", "contentinfo", "main", "navigation", "search"]:
                    implementation_data["landmarks"].append({
                            "url": url,
                        "landmark": landmark
                    })
            
            # Live-Regionen
            for element in accessibility.get("aria_labels", []):
                if element.get("aria_attrs", {}).get("aria-live"):
                    implementation_data["live_regions"].append({
                            "url": url,
                        "element": element
                    })
            
            # Beziehungen
            for element in accessibility.get("aria_labels", []):
                attrs = element.get("aria_attrs", {})
                if any(attr in attrs for attr in ["aria-controls", "aria-owns", "aria-describedby"]):
                    implementation_data["relationships"].append({
                            "url": url,
                        "element": element
                    })
            
            # Validierung
            for element in accessibility.get("aria_labels", []):
                attrs = element.get("aria_attrs", {})
                if attrs.get("aria-invalid") or attrs.get("aria-errormessage"):
                    implementation_data["validation"].append({
                        "url": url,
                        "element": element
                    })
                    
        return implementation_data

    # Neue Hilfsmethoden für die erweiterten Daten

    def _get_violations(self, accessibility_results: Dict[str, Any], violation_type: str) -> List[Dict[str, Any]]:
        """Extrahiert Verstöße eines bestimmten Typs aus den Accessibility-Ergebnissen"""
        if not accessibility_results or "violations" not in accessibility_results:
            return []
            
        return [v for v in accessibility_results.get("violations", []) if v.get("type") == violation_type]

    def _collect_text_alternatives(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Text-Alternativen für nicht-textuelle Inhalte"""
        alternatives_data = {
            "images_without_alt": [],
            "decorative_images": [],
            "complex_images": [],
            "svg_elements": [],
            "canvas_elements": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in alternatives_data.keys():
                if key in text_alts:
                    for item in text_alts[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        alternatives_data[key].append(item_copy)
                        
        return alternatives_data

    def _collect_icons_with_text(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Icons mit Text"""
        icons_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "icons_with_text" in text_alts:
                for icon in text_alts["icons_with_text"]:
                    icon_copy = icon.copy()
                    icon_copy["url"] = url
                    icons_data.append(icon_copy)
                    
        return icons_data

    def _collect_svg_elements(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über SVG-Elemente"""
        svg_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            text_alts = accessibility.get("text_alternatives", {})
            
            if "svg_elements" in text_alts:
                for svg in text_alts["svg_elements"]:
                    svg_copy = svg.copy()
                    svg_copy["url"] = url
                    svg_data.append(svg_copy)
                    
        return svg_data

    def _collect_landmarks(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über ARIA-Landmarks"""
        landmarks_data = {
            "navigation": [],
            "main": [],
            "complementary": [],
            "banner": [],
            "contentinfo": [],
            "search": [],
            "form": [],
            "region": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            landmarks = structure.get("landmarks", [])
            
            for landmark in landmarks:
                role = landmark.get("role")
                if role in landmarks_data:
                    landmark_copy = landmark.copy()
                    landmark_copy["url"] = url
                    landmarks_data[role].append(landmark_copy)
                    
        return landmarks_data

    def _collect_detailed_form_labels(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt detaillierte Informationen über Formularfeld-Labels"""
        form_labels = {
            "with_label": [],
            "with_aria_label": [],
            "with_placeholder": [],
            "without_label": [],
            "grouped_fields": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            forms = structure.get("forms", [])
            
            for form in forms:
                for field in form.get("fields", []):
                    field_data = {
                        "form_id": form.get("id"),
                        "field_id": field.get("id"),
                        "field_type": field.get("type"),
                        "url": url
                    }
                    
                    # Kategorisiere nach Labeltyp
                    if field.get("label"):
                        field_data["label"] = field["label"]
                        form_labels["with_label"].append(field_data)
                    elif field.get("aria_attributes", {}).get("aria-label"):
                        field_data["aria_label"] = field["aria_attributes"]["aria-label"]
                        form_labels["with_aria_label"].append(field_data)
                    elif field.get("placeholder"):
                        field_data["placeholder"] = field["placeholder"]
                        form_labels["with_placeholder"].append(field_data)
                    else:
                        form_labels["without_label"].append(field_data)
                        
                    # Gruppierte Felder
                    if field.get("grouped"):
                        group_data = field_data.copy()
                        group_data["group_info"] = field["grouped"]
                        form_labels["grouped_fields"].append(group_data)
                        
        return form_labels

    def _collect_text_spacing(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Textabstände"""
        spacing_data = {
            "line_height": [],
            "letter_spacing": [],
            "word_spacing": [],
            "text_indent": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            styling = page_data.get("styling", {})
            text_spacing = styling.get("text_spacing", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in spacing_data.keys():
                if key in text_spacing:
                    for value in text_spacing[key]:
                        spacing_data[key].append({
                            "value": value,
                            "url": url
                        })
                        
        return spacing_data

    def _collect_keyboard_traps(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über potenzielle Keyboard-Traps"""
        trap_data = {
            "custom_widgets": [],
            "modal_dialogs": [],
            "dropdown_menus": [],
            "negative_tabindex": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            keyboard_navigation = accessibility.get("keyboard_navigation", {})
            
            # Füge Daten mit URL-Referenz hinzu
            for key in trap_data.keys():
                if key in keyboard_navigation:
                    for item in keyboard_navigation[key]:
                        item_copy = item.copy()
                        item_copy["url"] = url
                        trap_data[key].append(item_copy)
                        
        return trap_data

    def _collect_focus_order(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über die Fokus-Reihenfolge"""
        focus_order_data = {
            "tab_order": [],
            "keyboard_navigation": [],
            "focus_visible": []
        }
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            tab_indices = accessibility.get("tab_index", [])
            
            # Sammle Tabindex-Informationen
            for tab_item in tab_indices:
                if not self._ensure_dict(tab_item):
                    continue
                
                item_copy = tab_item.copy()
                            item_copy["url"] = url
                focus_order_data["tab_order"].append(item_copy)
            
            # Fokus-Sichtbarkeit aus dem Styling
            styling = page_data.get("styling", {})
            css_analysis = styling.get("css_analysis", {})
            
            for style in css_analysis.get("inline_styles", []):
                if isinstance(style, dict) and any(focus_key in str(style) for focus_key in [":focus", "outline:", "box-shadow:"]):
                    focus_order_data["focus_visible"].append({
                        "url": url,
                        "style": style
                    })
            
            # Keyboard-Navigation aus Accessibility-Daten
            keyboard_nav = accessibility.get("keyboard_navigation", {})
            if keyboard_nav:
                focus_order_data["keyboard_navigation"].append({
                    "url": url,
                    "data": keyboard_nav
                })
                
        return focus_order_data
        
    def _collect_focus_order_data(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Alias für _collect_focus_order"""
        return self._collect_focus_order(crawler_data)

    def _collect_skip_links(self, crawler_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Sammelt Informationen über Skip-Links"""
        skip_links_data = []
        
        for url, page_data in crawler_data.get("data", {}).items():
            accessibility = page_data.get("accessibility", {})
            skip_links = accessibility.get("skip_links", [])
            
            for link in skip_links:
                if not self._ensure_dict(link):
                    continue
                
                link_copy = link.copy()
                link_copy["url"] = url
                skip_links_data.append(link_copy)
                
            # Suche auch nach potenziellen Skip-Links im HTML
            structure = page_data.get("structure", {})
            links = structure.get("links", [])
            
            for link in links:
                href = link.get("href", "")
                text = link.get("text", "")
                
                # Typische Skip-Link-Merkmale
                if (href and href.startswith("#") and 
                    any(keyword in text.lower() for keyword in ["skip", "überspringen", "zum inhalt", "to content", "main"])):
                    skip_links_data.append({
                        "url": url,
                        "href": href,
                        "text": text,
                        "detected_type": "content_skip_link"
                    })
                    
        return skip_links_data

    def _collect_page_titles(self, crawler_data: Dict[str, Any]) -> Dict[str, Any]:
        """Sammelt Informationen über Seitentitel"""
        titles_data = {
            "with_title": [],
            "without_title": [],
            "short_title": [],
            "duplicate_title": []
        }
        
        all_titles = {}  # Für die Erkennung von Duplikaten
        
        for url, page_data in crawler_data.get("data", {}).items():
            structure = page_data.get("structure", {})
            title_data = structure.get("title", {})
            
            if not self._ensure_dict(title_data):
                title_data = {}
                
            page_title = title_data.get("page_title")
            h1_title = title_data.get("h1_title")
            
            title_info = {
                "url": url,
                "page_title": page_title,
                "h1_title": h1_title
            }
            
            # Kategorisiere nach Titeltyp
            if page_title:
                # Speichere für Duplikatserkennung
                if page_title in all_titles:
                    all_titles[page_title].append(url)
                else:
                    all_titles[page_title] = [url]
                    
                if len(page_title) < 10:  # Kurzer Titel
                    titles_data["short_title"].append(title_info)
                else:
                    titles_data["with_title"].append(title_info)
            else:
                titles_data["without_title"].append(title_info)
                
        # Erkenne Duplikate
        for title, urls in all_titles.items():
            if len(urls) > 1:
                for url in urls:
                    titles_data["duplicate_title"].append({
                        "url": url,
                        "page_title": title,
                        "shared_with": [u for u in urls if u != url]
                    })
                    
        return titles_data

    # Weitere Hilfsmethoden für die anderen Kriterien...
    # [Hier würden die weiteren Implementierungen der Hilfsmethoden folgen] 